  <!DOCTYPE html>
  <html>
  <head>
  <title>docs/assurance-case.md</title>
  </head>
  <body>
<h1><a href="#badgeapp-security-its-assurance-case" aria-hidden="true" class="anchor" id="badgeapp-security-its-assurance-case"></a>BadgeApp Security: Its Assurance Case</h1>
<!-- SPDX-License-Identifier: (MIT OR CC-BY-3.0+) -->
<p>Security is important and challenging.<br />
This document describes why we think this software (the &quot;BadgeApp&quot;)<br />
is adequately secure.<br />
In other words, this document is the &quot;assurance case&quot; for the BadgeApp.<br />
This document is the result of continuous threat/attack modeling<br />
while the system is developed and maintained, and it is modified<br />
as the situation changes.<br />
For simplicity, this document also serves as detailed documentation of<br />
the security requirements, since in this case we found it<br />
easier to put them all in one document.</p>
<p>Sadly, perfection is rare; we really want your help.<br />
If you find a vulnerability, please see<br />
<a href="../CONTRIBUTING.html#how_to_report_vulnerabilities">CONTRIBUTING.md#how_to_report_vulnerabilities</a><br />
for how to submit a vulnerability report.<br />
For more technical information on the implementation, see<br />
<a href="implementation.html">implementation.md</a>.</p>
<p>You can see a video summarizing an older version of<br />
this assurance case (as of September 2017),<br />
along with some more general information about developing secure software:<br />
<a href="https://www.youtube.com/watch?v=5a5D4d6hcEY">&quot;How to Develop Secure Applications: The BadgeApp Example&quot; by David A. Wheeler, 2017-09-18</a>.<br />
For more information on developing secure software, see<br />
<a href="http://www.dwheeler.com/secure-programs/">&quot;Secure Programming HOWTO&quot; by David A. Wheeler</a>.<br />
<a href="https://www.ida.org/idamedia/Corporate/Files/Publications/IDA_Documents/ITSD/2019/P-9278.pdf"><em>A Sample Security Assurance Case Pattern</em> by David A. Wheeler (2018)</a><br />
shows how to create an assurance case for your project, using<br />
a version of this assurance case as an example.</p>
<p>We thank Scott Ankrum (MITRE)<br />
for analyzing an earlier version of this assurance case.<br />
He provided a number of helpful comments and provided a lot of feedback<br />
in how to convert its notation from the<br />
Claims, Arguments, and Evidence (CAE) notation to<br />
Structured Assurance Case Metamodel (SACM) notation.<br />
For his initial work in converting this assurance case to SACM notation, see<br />
<a href="https://www.researchgate.net/publication/351854207_BadgeApp_Assurance_Case_in_SACM_Notation"><em>BadgeApp Assurance Case in SACM Notation</em> by T. Scott Ankrum, The MITRE Corporation, May 2021</a>.</p>
<h2><a href="#assurance-case-summary" aria-hidden="true" class="anchor" id="assurance-case-summary"></a>Assurance case summary</h2>
<p>The following figures summarize why we think this application<br />
is adequately secure (more detail is provided in the rest of this document):<br />
The figures are simply a summary; the text below provides the details.</p>
<h3><a href="#top-level-assurance-case-in-sacm-notation" aria-hidden="true" class="anchor" id="top-level-assurance-case-in-sacm-notation"></a>Top level assurance case in SACM notation</h3>
<p>We are in the early stages of switching from<br />
Claims, Arguments, and Evidence (CAE) notation to the<br />
Structured Assurance Case Metamodel (SACM) graphical notation.<br />
We've converted the top level to SACM notation, and here it is:</p>
<p><img src="./assurance-case-toplevel-sacm.svg" alt="Assurance case summary" /></p>
<h3><a href="#assurance-case-in-cae-notation" aria-hidden="true" class="anchor" id="assurance-case-in-cae-notation"></a>Assurance case in CAE notation</h3>
<p>Currently our full assurance case is recorded in CAE notation.<br />
Here is the assurance case summary in CAE notation:</p>
<p><img src="./assurance-case.png" alt="Assurance case summary" /><br />
<img src="./assurance-case-lifecycle.png" alt="Assurance case in lifecycle" /><br />
<img src="./assurance-case-implementation.png" alt="Assurance case in implementation" /><br />
<img src="./assurance-case-other-lifecycle.png" alt="Assurance case in other processes" /></p>
<h3><a href="#overall-approach" aria-hidden="true" class="anchor" id="overall-approach"></a>Overall approach</h3>
<p>Our overall security approach is called<br />
defense-in-breadth, that is, we consider<br />
security (including security countermeasures) in all<br />
our relevant software life cycle processes (including<br />
requirements, design, implementation, and verification).<br />
In each software life cycle process we<br />
identify the specific issues that most need to be addressed,<br />
and then address them.</p>
<p>We do <em>not</em> use a waterfall model for software development.<br />
It's important to note that when we use the word <em>process</em> it<br />
has a completely different meaning from a <em>stage</em> (aka <em>phase</em>).<br />
Instead, we use the word &quot;process&quot; with its standard meaning in<br />
software and systems engineering, that is,<br />
a &quot;process&quot; is just a &quot;set of interrelated or interacting activities<br />
that transforms inputs into outputs&quot; (ISO/IEC/IEEE 12207:2017).<br />
In a waterfall model, these processes are done to completion<br />
in a strict sequence of stages (where each stage occurs for some<br />
period of time).<br />
That is, you create all of the requirements in<br />
one stage, then do all the design in the next stage, and so on.<br />
Winston Royce's paper &quot;Managing the Development of Large Software Systems&quot;<br />
(1970) notes that in software development this naive waterfall approach<br />
&quot;is risky and invites failure&quot; - in practice<br />
&quot;design iterations are never confined to the successive steps&quot;.<br />
We obviously <em>do</em> determine what the software will do differently<br />
(requirements), as well as design, implement, and verify it, so we<br />
certainly do have these processes.<br />
However, as with almost all real software development projects,<br />
we perform these processes in parallel, iterating and<br />
feeding back as appropriate.<br />
Trying to make decisions without feedback is extremely dangerous, e.g., see<br />
<a href="https://www.younglingfeynman.com/essays/physicsenvy">How Our Physics Envy Results In False Confidence In Our Organizations</a>.<br />
Each process is (notionally) run in parallel;<br />
each receives inputs and produces outputs.</p>
<p>To help make sure that we &quot;cover all important cases&quot;, most of<br />
this assurance case is organized by the life cycle processes<br />
as defined by ISO/IEC/IEEE 12207:2017,<br />
<i>Systems and software engineering - Software life cycle processes</i>.<br />
We consider every process, and include in the assurance case every<br />
process important to it.<br />
We don't claim that we conform to this standard, instead, we simply<br />
use the 12207 structure to help ensure that we've considered<br />
all of the lifecycle processes.</p>
<p>There are other ways to organize assurance cases, and we have taken<br />
steps to ensure that issues that would covered by them are indeed covered.<br />
An alternate way to view security issues is to discuss<br />
&quot;process, product, and people&quot;;<br />
we evaluate the product in the verification process, and<br />
the people in the human resources process.<br />
It is important to secure the enabling environments, including the<br />
development environments and test environment; it may not be obvious,<br />
but that is covered by the infrastructure management process.<br />
At the end we cover certifications and controls, which also help us<br />
reduce the risk of failing to identify something important.</p>
<p>The following sections are organized following the assurance case figures:</p>
<ul>
<li>We begin with the overall security requirements.<br />
This includes not just the high-level requirements in terms<br />
of confidentiality, integrity, and availability, but also<br />
access control in terms of identification, authentication (login),<br />
and authorization.  Authentication<br />
is a cross-cutting and critical supporting security mechanism, so<br />
it's easier to describe it all in one place.</li>
<li>This is followed in the software life cycle processes, focusing on<br />
the software lifecycle technical processes:<br />
design, implementation, integration and verification,<br />
transition (deployment) and operations, and maintenance.<br />
We omit requirements, since that was covered earlier.<br />
This is a merger of the second and third assurance case figures<br />
(implementation is shown in a separate figure because there is so much<br />
to it, but in the text we merge the contents of these two figures).</li>
<li>We then discuss security implemented by other life cycle processes,<br />
broken into the main 12207 headings:<br />
agreement processes, organizational project-enabling processes, and<br />
technical management processes.<br />
Note that the organizational project-enabling processes include<br />
infrastructure management (where we discuss security of the<br />
development and test environment)<br />
and human resource management (where we discuss the knowledge of<br />
the key people involved in development).</li>
<li>We close with a discussion of certifications and controls.<br />
Certification processes<br />
can help us find something we missed, as well as provide confidence<br />
that we haven't missed anything important).<br />
Note that the project receives its own badge<br />
(the CII best practices badge),<br />
which provides additional evidence that it applies best practices<br />
that can lead to more secure software.<br />
Similarly, selecting IA controls can help us review important issues<br />
to ensure that the system will be adequately secure in its intended<br />
environment (including any compensating controls added to its environment).<br />
We controls in the context of the<br />
<a href="https://www.cisecurity.org/controls/">Center for Internet Security (CIS) Controls</a><br />
(aka critical controls).</li>
</ul>
<p>We conclude with a short discussion of residual risks,<br />
describe the vulnerability report handling process, and make<br />
a final appeal to report to us if you find a vulnerability.</p>
<p>In this assurance case we typically point to source code or tests as<br />
evidence, and not the results of the tests themselves. We do not<br />
ship to production unless tests pass, so there is usually no reason to<br />
see the test results unless a test fails.<br />
That said, the test results for the master branch<br />
are available if desired at:<br />
<a href="https://app.circleci.com/pipelines/github/coreinfrastructure/best-practices-badge?branch=master">https://app.circleci.com/pipelines/github/coreinfrastructure/best-practices-badge?branch=master</a></p>
<p>(Note to editors: to edit the figures above, edit the .odg file, then<br />
export to .png so that it can viewed on GitHub.)</p>
<h2><a href="#cae-notation" aria-hidden="true" class="anchor" id="cae-notation"></a>CAE notation</h2>
<p>Our full assurance case is currently described in CAE notation.</p>
<p>Claims, Arguments and Evidence (CAE) notation<br />
is a simple notation often used for assurance cases.<br />
In CAE notation,<br />
Ovals are claims or sub-claims, while rounded rectangles are the supporting<br />
arguments justifying the claims.<br />
Evidence, where shown, are in rectangles.</p>
<p>We do not show most evidence in the figures, but provide the evidence in<br />
the supporting text below instead, because large figures are time-consuming<br />
to edit and for our purposes providing most evidence only in the supporting<br />
test is adequate.</p>
<h2><a href="#structured-assurance-case-metamodel-sacm-graphical-notation" aria-hidden="true" class="anchor" id="structured-assurance-case-metamodel-sacm-graphical-notation"></a>Structured Assurance Case Metamodel (SACM) Graphical Notation</h2>
<p>Some figures of this assurance case uses a subset of the<br />
[Object Management Group (OMG) Structured Assurance Case Metamodel (SACM)](Structured Assurance Case Metamodel (SACM))<br />
graphical notation.<br />
The OMG specification, which is publicly available, defines SACM in detail.<br />
In this section we'll explain the subset of SACM<br />
graphical notation and conventions that we use.</p>
<p>Assurance cases typically use one of three graphical notations:<br />
<a href="https://www.adelard.com/asce/choosing-asce/cae.html">Claims- Arguments- Evidence (CAE) notation</a>,<br />
Goal Structuring Notation (GSN), or the SACM graphical notation.<br />
The original BadgeApp assurance case used the<br />
CAE notation because it is simple and the SACM graphical notation did not exist.<br />
However, the SACM specification version 2.1 added in 2020<br />
a graphical notation that has many advantages, so we have switched to SACM.<br />
Later in this document we'll discuss the advantages of SACM.</p>
<h3><a href="#explanation-of-sacm-notation-subset" aria-hidden="true" class="anchor" id="explanation-of-sacm-notation-subset"></a>Explanation of SACM Notation Subset</h3>
<p>Here is the subset of the SACM graphical notation that we use:</p>
<ol>
<li><em>Claim</em>.<br />
A claim is a statement that can be either true or false (not both).<br />
A claim is represented as a rectangle (we fill them with light blue 3),<br />
A claim that supports another claim is also called a subclaim.<br />
It is equivalent to the CAE Claim and GSN Goal.</li>
<li><em>ArtifactReference</em>, which is used for  <em>evidence</em>.<br />
An ArtifactReference refers to some artifact (such as a piece of information),<br />
and is represented as a shadowed rectangle.<br />
When an ArtifactReference is used to support a claim,<br />
which is the only way we use them, they're also called evidence.<br />
The official SACM graphical notation includes an angled arrow in its icon,<br />
but our tools don't easily support that.<br />
As we use them this is equivalent to the CAE Evidence and GSN Solution.</li>
<li><em>ArgumentReasoning</em> aka <em>argument</em>.<br />
An argument explains why the supporting claims and evidence justify<br />
the claim.<br />
It is represented as a half-open rectangle<br />
(we fill them with light magenta 4).</li>
<li><em>AssertedInference</em> and <em>AssertedEvidence</em>, aka kinds of <em>relationships</em>.<br />
SACM terminology is that an AssertedInference shows that a claim supports<br />
another claim, and an AssertedEvidence shows that an ArtifactReference<br />
(evidence) supports a claim.<br />
But they have the same graphical representation, and<br />
we'll just call both relationships.<br />
They are shown as directed lines with a bigdot<br />
and an arrowhead pointing to the claim(s) or relationship being justified.<br />
SACM relationships (AssertRelationships) can do more,<br />
but we do not use the other forms.</li>
<li><em>ArgumentPackage</em> aka <em>package</em>.<br />
An argument package is a grouping of argumention elements.<br />
This lets us break the information into multiple pages.<br />
We show this as a scroll; the official SACM graphical symbol is complicated<br />
and not supported by our drawing tool.<br />
It is equivalent to the GSN Module.</li>
<li><em>asCited Claim</em>.<br />
An asCited Claim is a claim expanded elsewhere, that is, a cross-reference.<br />
Its description text shows its containing package, followed by the<br />
claim id in square brackets.<br />
This is represented as a bracketed rectangle.</li>
</ol>
<p><img src="./sacm.svg" alt="SACM notation summary" /></p>
<p>The text shows an ID and colon (in bold), followed by whitespace and<br />
its description.</p>
<p>The SACM graphical notation includes many other features (such as<br />
contexts and other kinds of relationships) that we don't use.<br />
In SACM these elements can have &quot;notes&quot; attached to them; the equivalent<br />
to notes is the text in this document.<br />
The notes may refer to added evidence and/or arguments.<br />
We don't use many other constructs, such as SACM contexts.<br />
The paper<br />
&quot;A Visual Notation for the Representation of Assurance Cases using SACM&quot;<br />
(2020) provides more information, but unfortunately that paper<br />
is not publicly available.</p>
<p>In the rest of this document we will often use the term &quot;argument&quot;<br />
for SACM’s ArgumentReasoning, and &quot;evidence&quot; for ArtifactReference,<br />
because these are simpler terms.</p>
<h3><a href="#conventions" aria-hidden="true" class="anchor" id="conventions"></a>Conventions</h3>
<p>Here are some conventions we use:</p>
<ul>
<li>Our convention is that the argument description completes the phrase<br />
&quot;The claim is justified by these subclaims/evidence because (TEXT)&quot;.</li>
<li>Where possible, the first or second word in the description<br />
is distinctive.  That makes it easier to see what is most important.<br />
We try to put phrases like &quot;is secure&quot; or &quot;is countered&quot; last;<br />
those aren't distinctive, since many<br />
claims are about security or about countering something.</li>
<li>We structure this as primarily claims and subordinate subclaims,<br />
instead of as arguments and subordinate arguments, per 2020 feedback from<br />
MITRE. This is an improvement; since claims are each true/false statements,<br />
the relationship between them is usually much clearer doing it this way.</li>
</ul>
<h2><a href="#security-requirements" aria-hidden="true" class="anchor" id="security-requirements"></a>Security Requirements</h2>
<p>We believe the basic security requirements have been identified and met,<br />
as described below.<br />
The security requirements identified here were developed through our<br />
requirements process, which merges<br />
three related processes in ISO/IEC/IEEE 12207<br />
(business or mission analysis, stakeholder needs and requirements definition,<br />
and systems/software requirements definition).</p>
<p>Security requirements are often divided into three areas called the<br />
&quot;CIA triad&quot;: confidentiality, integrity, and availability.<br />
We do the same here below, including a discussion of why we<br />
believe those requirements are met.<br />
These justifications depend on other processes<br />
(e.g., that the design is sound, the implementation is not vulnerable,<br />
verification is adequate), which we will justify later.<br />
This is followed by a discussion of access control, and then<br />
a discussion showing that the<br />
the assets &amp; threat actors have been identified &amp; addressed.</p>
<p>See the design section for discussion about why we believe is not possible<br />
to bypass the mechanisms discussed below.</p>
<h3><a href="#confidentiality" aria-hidden="true" class="anchor" id="confidentiality"></a>Confidentiality</h3>
<h4><a href="#user-privacy-maintained" aria-hidden="true" class="anchor" id="user-privacy-maintained"></a>User privacy maintained</h4>
<p>One of our key requirements is to<br />
&quot;protect users and their privacy&quot;.<br />
Here is a brief discussion on how we do that.</p>
<p>First, the basics.<br />
We work hard to comply with the<br />
EU General Data Protection Regulation (GDPR), which has many requirements<br />
related to privacy.<br />
We have a separate document that details how we<br />
implement privacy in the CII Best Practices Badge site and comply with<br />
the GDPR:<br />
<a href="https://docs.google.com/document/d/1qarSkCJacjoMeu1k6p5JQXvPt-0xUqzKy3OW8zmGvpg">Privacy in the CII Best Practices site, focusing on the GDPR</a>.<br />
As discussed later, non-public data is kept confidential<br />
both at rest and in motion<br />
(in particular, email addresses are protected).</p>
<p>Part of our privacy requirement is that we<br />
&quot;don't expose user activities to unrelated sites (including social media<br />
sites) without that user's consent&quot;;<br />
here is how we do that.</p>
<p>We must first define what we mean by an unrelated site.<br />
A &quot;related&quot; site is a site that we are directly using to provide our service,<br />
in particular our cloud provider (Heroku which runs on<br />
Amazon's EC2 cloud-computing platform), CDN provider (Fastly),<br />
authorization and avatar services provider (GitHub),<br />
external avatar services (Gravatar),<br />
and logging / intrusion detection service.<br />
As a practical matter, related sites must (under various circumstances)<br />
receive some information about the user (at least that the user<br />
is trying to do something).<br />
This is true for all websites, so it's true for our site as well.<br />
In those cases we have selected partners we believe are trustworthy, and<br />
we have some kind of relationship with them.</p>
<p>However, there is no reason unrelated sites<br />
<em>must</em> see what our users are doing,<br />
so we take many steps to prevent unrelated sites from<br />
learning about our users' activities (and thus maintaining user privacy):</p>
<ul>
<li>We directly serve all our own assets ourselves,<br />
including JavaScript, images, and fonts.<br />
In particular, we do not have <em>any</em> embedded automatically-downloaded<br />
references (transclusions)<br />
in our web pages to external JavaScript or fonts.<br />
Since we serve these assets ourselves, and not via external<br />
third parties, external sites never receive any request from a user<br />
when they view our pages.<br />
As a result, user privacy is maintained: what a user views on our site<br />
is never revealed by our actions to unrelated sites.<br />
This also aids security; even if an attacker subverts some other site's<br />
JavaScript or font, that will not directly affect us because we do not embed<br />
references some other site's JavaScript or font in our web pages.<br />
Many sites don't do this and should probably consider it.<br />
This policy is enforced by our CSP policy.</li>
<li>We do not serve ads and we plan to have no ads in the future.<br />
That said, if we ever did serve ads, we expect that we<br />
would also serve them from our site, just like any other asset, to<br />
ensure that third parties did not receive unauthorized information.</li>
<li>We do not use any web analytics service that uses tracking codes or<br />
external assets.<br />
We log and store logs using only services we control or have a direct<br />
partnership with.</li>
<li>The email we send is privacy-respecting.<br />
The email contents we send do not have img links (which might expose<br />
when an email is read). In some cases we have hyperlinks<br />
(e.g., to activate a local account), but those links go directly back<br />
to our site for that given purpose, and do not reveal information to<br />
anyone else.<br />
You need to configure the MTA used to send email before you can use it.<br />
We used to use SendGrid to send email; we have specifically configured the<br />
<a href="https://sendgrid.com/docs/ui/account-and-settings/tracking/">SendGrid X-SMTPAPI header to disable all of its trackers we know of</a>,<br />
which are clicktrack, ganalytics, subscriptiontrack, and opentrack.<br />
For example, we have never used ganalytics, but by expressly disabling it,<br />
it will stay disabled even if SendGrid decided to enable it by default<br />
in the future.</li>
<li>We do have links to social media sites (e.g., from the home page), but we<br />
do this in a privacy-respecting manner.<br />
It would be easy to use techniques like embedding images<br />
from external (third party) social media sites,<br />
but we intentionally do not do that, because that would expose to an<br />
external unrelated site what our users are doing without their knowledge.<br />
We instead use the approach described in<br />
<a href="https://jonsuh.com/blog/social-share-links/#use-share-urls">&quot;Responsible Social Share Links&quot; by Jonathan Suh (March 26, 2015), specifically using share URLs</a>.<br />
In this approach, if a user does not press the link,<br />
the social media site never receives any information.<br />
Instead, a social media site<br />
<em>only</em> receives information when the user takes a direct action to<br />
request it (e.g., a click), and that site only receives information from<br />
the specific user who requested it.</li>
</ul>
<p>Note that user avatar images are handled specially. We consider<br />
the few avatar-serving domains that we use as related sites.<br />
This issue may not be obvious, so here we'll explain it further.<br />
A user can choose a representative avatar<br />
(currently via GitHub or Gravatar).<br />
Anyone who requests that user's information page will<br />
receive an <code>img</code> reference to that user-selected avatar so that<br />
the requestor can see it.<br />
External avatar images are only shown from specific domains<br />
('secure.gravatar.com' or 'avatars.githubusercontent.com'), they are<br />
only included if the user has an avatar, and they are only shown to<br />
others through this mechanism if that user's information was requested.<br />
This functionality is useful, because these images can help others remember<br />
who the user is.</p>
<p>We have considered ways to further limit information sharing with avatar<br />
services even though they are related sites (and thus we do not <em>have</em><br />
to limit information sharing any further).<br />
We have had some success, but current law and technology provide challenges.<br />
We could download these images and re-serve them (such as via a proxy),<br />
but copying or proxying the images<br />
using our own site might be considered a copyright violation<br />
and would also impose the need for significant extra resources.<br />
Thus, since we do not serve avatars ourselves, we must direct requesters<br />
to them, so at the very least the requestor's externally-visible IP address<br />
must be visible to the external avatar service (so the image can be provided).<br />
To provide additional privacy, we would like to also<br />
limit requestor headers and third-party cookies when using third-party<br />
avatar services<br />
(since these are the primary mechanisms that reveal more information<br />
about the requestor to the third party avatar service).<br />
Here is our current state:</p>
<ul>
<li>We think we have a decent solution for limiting<br />
requestor headers from being sent to avatar services.<br />
We have added the <code>referrerpolicy=&quot;no-referrer&quot;</code> attribute to the image<br />
as discussed in the<br />
<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/img">Mozilla img documentation</a>.<br />
While this attribute is technically experimental,<br />
<a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLImageElement/referrerPolicy">the referrerpolicy attribute on images is widely supported</a>,<br />
including by Chrome, Firefox, and Opera.</li>
<li>Unfortunately, we have not found a <em>good</em> way to prevent<br />
third-party cookies from being sent to avatar services.<br />
<a href="https://stackoverflow.com/questions/51549390/how-to-disable-third-party-cookie-for-img-tags">There are discussions on how to disable third party cookies for img tags</a>,<br />
but currently-known mechanisms are very complex, require inline CSS<br />
invocations, and have dubious reliability.<br />
We hope that future web standards will add the ability to easily<br />
prevent the unnecessary revelation of third-party cookies.</li>
</ul>
<p>Note that we consider that things are very different<br />
when a user actively clicks on a hypertext link to go to a different web site.<br />
In this case, the user has <em>actively</em> selected to visit that different web site,<br />
and thus expressly consented to the usual actions that occur when visiting<br />
a different web site.<br />
The different web site must know the IP address of the user anyway<br />
(to send the data), and any cookie communications with that site involve<br />
that other site.<br />
We do allow referrer information to be sent in this case.<br />
When a user actively selects a hypertext link, it is normal web behavior<br />
for the receiving site to be provided with information on the referrer<br />
via the &quot;referer&quot; (sic) HTTP header as specified in<br />
<a href="https://tools.ietf.org/html/rfc1945#page-44">RFC 1945 (HTTP 1.0) from May 1996</a>.<br />
This referrer information reports where the user &quot;came from&quot;.<br />
This information is useful for many circumstances, including notifying<br />
recipients that people are discovering their site using our site.<br />
It would be <em>possible</em> for us to<br />
<a href="https://geekthis.net/post/hide-http-referer-headers/">disable sending referrer information</a><br />
(e.g., by using <code>rel=&quot;noreferrer&quot;</code> in hypertext links or<br />
by setting a referrer policy via HTTP or the meta tag), but this<br />
would inhibit normal default web behavior.<br />
However, when users expressly choose to click on a link from our site,<br />
there is express consent to visit the other site,<br />
so we do not see this an issue.<br />
In addition, users who do not want to share referrer information can<br />
configure their browser to omit referrer information; this would<br />
be a much better choice for users who do not want referrer information<br />
to be shared.<br />
Information on how to disable referrer heading information is<br />
available from multiple sources, e.g.,<br />
<a href="https://www.addictivetips.com/vpn/change-referer-header-settings/">&quot;How to Change Referer Header Settings: Why It’s Useful&quot; by John Anthony</a>,<br />
<a href="https://www.technipages.com/firefox-enable-disable-referrer">&quot;Turn Referrer Headers On or Off in Firefox&quot; by Mitch Bartlet</a>,<br />
and<br />
<a href="https://proprivacy.com/guides/change-referer-header-settings">&quot;Why Should you Change your Referer Header settings &amp; How to do it&quot; by Douglas Crawford</a>.</p>
<p>Of course, to access any site on the Internet<br />
the user must use various services and<br />
computers, and some of those could be privacy-exposing.<br />
For example, the user must make a request to a DNS service to find our<br />
service, and user requests must transit multiple Internet routers.<br />
We cannot control the systems that users choose to use; instead, we ensure that<br />
users can choose what services and computers they will trust.<br />
The BadgeApp does not filter out any particular source<br />
(other than temporary blocks if the source becomes a source of attack).<br />
Therefore, users who do not want their activities monitored<br />
could choose to use a network and computer they trust,<br />
a Virtual Private Network (VPN), or an anonymity network such as Tor<br />
to provide additional privacy when they interact with the BadgeApp.</p>
<h4><a href="#almost-all-data-is-not-confidential" aria-hidden="true" class="anchor" id="almost-all-data-is-not-confidential"></a>Almost all data is not confidential</h4>
<p>We try to store as little confidential information as we reasonably can,<br />
as this limits the impact of any confidentiality breach.<br />
Almost all data we collect is considered public, e.g., all project data,<br />
who owns the project information, and GitHub user names.<br />
Therefore, we don't need to keep those confidential.</p>
<p>For each user account we store some other data.<br />
Some we present to the public, such as claimed user name,<br />
creation time, and edit times; we present these as we consider that<br />
public information.<br />
We do not present the user's preferred locale to the public, under the<br />
theory that we don't know of a reason someone else would<br />
have a legitimate reason to know that.<br />
However, all of this is not considered sensitive data and it is certainly<br />
not identifying information, so we would not consider it a breach<br />
if someone else got this information (such as the preferred locale).</p>
<p>Non-public data is kept confidential.<br />
In our case, the non-public data that must be kept confidential<br />
are the user passwords, the &quot;remember me&quot; token (login nonce)<br />
if the user has enabled the remember me function, and user email addresses.<br />
We <em>do</em> consider this data higher-value and protect them specially.</p>
<h4><a href="#user-passwords" aria-hidden="true" class="anchor" id="user-passwords"></a>User passwords</h4>
<p>User passwords for local accounts are only stored on the server as<br />
iterated per-user salted hashes (using bcrypt), and thus cannot<br />
be retrieved in an unencrypted form later on from a recorded database.</p>
<p>Any user password sent to the system is sent by the application router<br />
to the &quot;update&quot; method of the user controller<br />
(<code>app/controllers/users_controller.rb</code>).  The update method invokes the<br />
&quot;save&quot; method of the user model (<code>app/models/user.rb</code>).<br />
The user model includes the standards Rails<br />
<code>has_secure_password</code> request, which tells the system to save<br />
the password after it has been encrypted with bcrypt<br />
(instead of storing it directly), and to later do comparisons using<br />
only the bcrypted value.</p>
<p>Note that no normal request can later retrieve a password, because<br />
it is immediately encrypted on change or check and the original<br />
unencrypted password is discarded.<br />
The unencrypted email address may remain for a while in server memory<br />
until that memory is recycled, but we assume that the underlying<br />
system will protect memory during the time before it is garbage-collected<br />
and reused.</p>
<p>To help users avoid the <em>worst</em> passwords, we check proposed passwords<br />
against a list of bad passwords and don't allow passwords to change to them.<br />
This complies with NIST Special Publication 800-63B section 5.1.1.2.<br />
That list is long, so for speed we keep this list in the database<br />
(if we kept it in memory it would use a lot of memory).<br />
We don't trust long-term storage, so it's vital that we <em>never</em> record<br />
in logs a query that includes an unencrypted password.<br />
Also, while we trust our database and its connections, we want to limit<br />
privileges where we can. Thus, we take two steps to protect passwords<br />
while we check them against the bad password list in the database:</p>
<ol>
<li>We disable the bad password check if the log level is set to debug<br />
(which logs SQL queries) and we aren't running a test.<br />
It's better to disable the bad password check than to risk exposing<br />
a password. When this occurs, a warning it noted in the log.<br />
Note that the production environments settings do <em>not</em> use log level<br />
debug by default anyway, but we want to make sure it won't happen.</li>
<li>Instead of storing the bad passwords directly, we stored keyed HMAC<br />
values of them, and the new password must <em>also</em> be converted into a<br />
keyed HMAC for comparison. The key is a secret for the application.<br />
This means that even if an attacker gains control over the database<br />
and/or can read communications to it (which we assume they can't do),<br />
the attacker would have to use a brute-force to find the key<br />
(we recommend 512 bits), <em>then</em> use that key to compute HMACs to attempt<br />
to discover a user's password. Rekeying the bad password database<br />
is trivial, when desired. A new key can be set in <code>BADGEAPP_BADPWKEY</code><br />
and then you can run <code>rake update_bad_password_db</code> (it takes a few minutes).<br />
We do this to protect passwords used for queries between the application<br />
and the database, to counter someone snooping it.<br />
We <em>never</em> store the HMAC of the password anywhere. We instead use<br />
bcrypt for stored passwords (it's designed for that).<br />
Strictly speaking using HMAC isn't necessary, since we don't store it,<br />
but providing another layer of protections for passwords seemed appropriate.<br />
Theoretically a user could choose another password that isn't<br />
on the bad password list but matches its HMAC; that's so astronomically<br />
unlikely that it won't happen, and even if it did, it would just mean<br />
that the user would have to choose a different password.</li>
</ol>
<h4><a href="#remember-me-token" aria-hidden="true" class="anchor" id="remember-me-token"></a>Remember me token</h4>
<p>Users may choose to &quot;remember me&quot; to automatically re-login on<br />
that specific browser if they use a local account.<br />
This is done by enabling the &quot;remember me&quot; checkbox when logging in to<br />
a session.<br />
If a user does enable &quot;remember me&quot; we implement automatic<br />
login when the user makes later requests.  This is implemented using a<br />
cryptographically random nonce stored in the user's web browser<br />
cookie store as a permanent cookie.<br />
Note that this nonce does not include the user's original password.<br />
On the server side this nonce is encrypted via bcrypt just like<br />
user passwords are stored.</p>
<p>Here is how we do this: Any attempt to login is routed to the<br />
&quot;new&quot; method of the sessions controller in<br />
<code>app/controllers/sessions_controller.rb</code>, which calls method<br />
<code>local_login</code>, and if login is successful it<br />
calls <code>local_login_procedure</code>.<br />
If the user selected <code>remember_me</code>, the<br />
<code>local_login_procedure</code> will call the <code>remember</code> method in<br />
the user model (in <code>app/models/user.rb</code>) to create and store the<br />
<code>remember_token</code> in the user's cookie store and the corresponding<br />
bcrypted value on the server.<br />
The user may log out later (often by having their log in session time out).</p>
<p>Whenever the system needs to determine who the current user is,<br />
it calls method <code>current_user</code> (in <code>app/helpers/sessions_helper.rb</code>).<br />
If the user is not logged in, but has a <code>remember_me</code> token that<br />
matches the hashed token on the server, this method automatically<br />
logs the user back in.<br />
See the section on authentication for more information.</p>
<p>The system does not have the unencrypted <code>remember_token</code> for any<br />
given user (only its bcrypted form), so the system cannot later reveal the<br />
<code>remember_token</code> to anyone else.</p>
<p>In file <code>test/integration/users_login_test.rb</code> we verify that the<br />
password is not stored as cleartext in the user cookie.</p>
<h4><a href="#email-addresses" aria-hidden="true" class="anchor" id="email-addresses"></a>Email addresses</h4>
<p>Email addresses are only revealed to the owner of the email address and to<br />
administrators.</p>
<p>We must store email addresses,<br />
because we need those for various purposes.<br />
In particular, we must be able to contact badge entry owners<br />
to discuss badge issues (e.g., to ask for clarification).<br />
We also user email addresses as the user id for &quot;local&quot; accounts.<br />
Since we must store them,<br />
we strive to not reveal user email addresses to others<br />
(with the exception of administrators, who are trusted and thus<br />
can see them).</p>
<p>Here are the only ways that user email addresses can be revealed<br />
(use <code>grep -Ri 'user.*\.email' ./</code> to verify):</p>
<ul>
<li>Mailers (in <code>app/mailers/</code>).  The application sometimes sends email, and<br />
in all cases email is sent via mailers.  Unsurprisingly, we need destination<br />
email addresses to send email.  However, in all cases we only<br />
send emails to a single user, with possible &quot;cc&quot; or &quot;bcc&quot; to a<br />
(trusted) administrator.  That way, user email addresses cannot leak<br />
to other users via email.  This can be verified by examining the<br />
mailers in directory <code>app/mailers/</code> and their corresponding views in<br />
<code>app/views/*_mailer/</code>. Even the rake task <code>mass_email</code><br />
(defined in file lib/tasks/default.rake),<br />
which can send a message such as &quot;we have been breached&quot; to<br />
all users, sends a separate email to each user using a mailer.<br />
A special case is when a user changes their email address: in that case,<br />
information is sent to both email addresses, but technically that is still<br />
an email to a single user, and this is only done when someone is logged<br />
in with authorization to change the user email address.</li>
<li>The only <em>normal</em> way to display user email addresses is to invoke<br />
a view of a user or a list of users.  However, these invoke<br />
user views defined in <code>app/views/users/</code>, and all of these views only<br />
provide a user email address if the current user is the user<br />
being displayed<br />
or the current user is an administrator.  This is true for views in both<br />
HTML and JSON formats.<br />
In addition, while we directly display the email address when local users<br />
are editing it (we must, so that users can change it), when user<br />
records are shown (not edited) the email address is only available via<br />
a hypertext link, and not directly displayed on the screen, to reduce<br />
the risk of revealing an email address while using sharing a screen.<br />
The following automated tests verify that email addresses<br />
are not provided without authorization:
<ul>
<li><code>should NOT show email address when not logged in</code></li>
<li><code>JSON should NOT show email address when not logged in</code></li>
<li><code>should NOT show email address when logged in as another user</code></li>
<li><code>JSON should NOT show email address when logged in as another user</code></li>
</ul>
</li>
<li>The <code>reminders_summary</code> view in<br />
<code>app/views/projects/reminders_summary.html.erb</code><br />
does display user email addresses, but this is only displayed when a<br />
request is routed to the <code>reminders_summary</code> method of the projects controller<br />
(<code>app/controllers/projects_controller.rb</code>), and this method only displays<br />
that view to administrators.<br />
This is verified by the automated test<br />
<code>Reminders path redirects for non-admin</code>.</li>
<li>As a special case, a user email address is included as a hidden field in<br />
a local user password reset in <code>app/views/password_resets/edit.html.erb</code>.<br />
However, this is only displayed if the user is routed to the &quot;edit&quot;<br />
method of <code>app/controllers/password_resets_controller.rb</code> and successfully<br />
meets two criterion (configured using <code>before_action</code>):<br />
<code>require_valid_user</code> and <code>require_unexpired_reset</code>.<br />
The first criterion requires that the user be activated and provide the<br />
correct reset authentication token that was emailed to the user;<br />
anyone who can do this can already receive or intercept that user's email.<br />
The need for the correct authentication token<br />
is verified by the automated test <code>password resets</code>.</li>
</ul>
<p>As documented in CONTRIBUTING.md, we forbid including email<br />
addresses in server-side caches, so that accidentally sharing the<br />
wrong cache won't reveal email addresses.<br />
Most of the rest of this document describes the other<br />
measures we take to prevent turning unintentional mistakes<br />
into exposures of this data.</p>
<p>Note: As discussed further in the later section on &quot;Encrypted email addresses&quot;,<br />
we also encrypt the email addresses using AES with 256-bit keys in<br />
GCM mode ('aes-256-gcm').  We also hash the email addresses, so they<br />
can be indexed, using the hashed key algorithm PBKDF2-HMAC-SHA256.<br />
These are strong, well-tested algorithms.<br />
We encrypt email addresses, to provide protection for data at rest,<br />
and never provide the keys to the database system<br />
(so someone who can only see what the database handles, or can<br />
get a copy of it, will not see sensitive data including<br />
raw passwords and unencrypted email addresses).<br />
These are considered additional hardening measures, and so are<br />
discussed further in the section on hardening.</p>
<p>Password reset requests (for local users) trigger an email,<br />
but that email is sent to the address as provided by the original account;<br />
emails are <em>not</em> sent to whatever email address is provided by the<br />
reset requestor (who might be an attacker).<br />
These email addresses match in the sense of <code>find_by</code>, which is a<br />
case-insensitive match, but since it is sometimes possible for an attacker<br />
to create another email account that &quot;matches&quot; in a case-insensitive way<br />
to an existing account, we always use the known-correct email address.<br />
You can verify this by reviewing<br />
<code>app/controllers/password_resets_controller.rb</code>.<br />
This approach completely counters the attack described in<br />
<a href="https://eng.getwisdom.io/hacking-github-with-unicode-dotless-i/">Hacking GitHub with Unicode's dotless 'i'</a>.</p>
<p>The presence or absence of an email address is not revealed by the<br />
authentication system (countering enumeration or verification attacks):</p>
<ol>
<li>Local account creation always reports that the account must be<br />
verified by checking the delivered email, whether or not the email<br />
account exists. Thus, attempting to create a local account won't<br />
reveal if the account exists to others.</li>
<li>Password reset requests do <em>not</em> vary depending<br />
on whether or not the email address is present as a local account.</li>
<li>Failed login requests for local accounts simply reports that<br />
the login failed; they do not indicate if the email address is present.</li>
</ol>
<h4><a href="#https" aria-hidden="true" class="anchor" id="https"></a>HTTPS</h4>
<p>HTTPS (specifically the TLS protocol)<br />
is used to encrypt all communications between users<br />
and the application.<br />
This protects the confidentiality and integrity of all data in motion,<br />
and provides confidence to users that they are contacting the correct server.</p>
<p>We force the use of HTTPS by setting<br />
<code>config.force_ssl</code> to <code>true</code> in the<br />
<code>config/environments/production.rb</code> (the production configuration).<br />
This enables a number of hardening mechanisms in Rails, including<br />
TLS redirection (which redirects HTTP to HTTPS).<br />
(There is a debug mode to disable this, <code>DISABLE_FORCE_SSL</code>,<br />
but this is not normally set in production and can only be set by<br />
a system administrators with deployment platform access.)</p>
<p>As discussed in the hardening section<br />
&quot;Force the use of HTTPS, including via HSTS&quot; (below), we take a number of<br />
additional steps to try to make users always use HTTPS.<br />
We also use <a href="#online-checkers">online checkers</a> (discussed below)<br />
to verify that our TLS configuration is secure in production.</p>
<h3><a href="#integrity" aria-hidden="true" class="anchor" id="integrity"></a>Integrity</h3>
<p>As noted above,<br />
HTTPS is used to protect the integrity of all communications between<br />
users and the application, as well as to authenticate the server<br />
to the user.</p>
<h4><a href="#data-modification-requires-authorization" aria-hidden="true" class="anchor" id="data-modification-requires-authorization"></a>Data modification requires authorization</h4>
<p>Data modification requires authorization.</p>
<p>Here we describe how these authorization rules are enforced.<br />
We first discuss how to modify data through the BadgeApp application,<br />
and then note that data can also be modified by modifying it via the<br />
underlying database and platform.<br />
For more about the authorization rules themselves,<br />
see the section on authorization.<br />
Note that gaining authorization first requires logging in<br />
(which in turn requires both identification and authentication).</p>
<p>The only kinds of data that can be modified involve a project or a user,<br />
and this data can only be modified through the application as follows:</p>
<ul>
<li>Project:<br />
Any project edit or deletion request is routed to the appropriate<br />
method in the projects controller in<br />
<code>app/controllers/projects_controller.rb</code>.<br />
Users cannot invoke any other method to modify a project other than<br />
the four methods corresponding to the requests identified below, and<br />
these cannot be executed unless the appropriate authentication check<br />
has succeeded:
<ul>
<li>In the case of an <code>edit</code> or <code>update</code> request, there is a <code>before_action</code><br />
that verifies that the request is authorized using the check method<br />
<code>can_edit_else_redirect</code>.<br />
(Note: technically only <code>update</code> needs authentication, since<br />
<code>edit</code> simply displays a form to fill out.  However, to reduce<br />
user confusion, we prevent <em>displaying</em> a form for editing data<br />
unless the user is authorized to later perform an update.)<br />
This inability to edit a project without authorization<br />
is verified by automated tests<br />
<code>should fail to update project if not logged in</code> and<br />
<code>should fail to update other users project</code>.</li>
<li>In the case of a <code>delete_form</code> or <code>destroy</code> request,<br />
there is a <code>before_action</code><br />
that verifies that the request is authorized using the check method<br />
<code>can_control_else_redirect</code>.<br />
(Note: Again, technically only <code>destroy</code> needs authentication, but<br />
to reduce user confusion we will not even display the form for destroying<br />
a project unless the user is authorized to destroy it.)<br />
This inability to destroy a project without authorization<br />
is verified by automated tests<br />
<code>should not destroy project if no one is logged in</code> and<br />
<code>should not destroy project if logged in as different user</code>.</li>
</ul>
</li>
<li>User:<br />
Any user edit or deletion request is routed to the appropriate<br />
method in the user controller in<br />
<code>app/controllers/users_controller.rb</code>.<br />
These cannot be executed unless the appropriate authentication check<br />
has succeeded.<br />
In the case of an <code>edit</code> or <code>update</code> or <code>destroy</code> request,<br />
there is a <code>before_action</code><br />
that verifies that the request is authorized using the check method<br />
<code>redir_unless_current_user_can_edit</code>.<br />
Users cannot invoke any other method to modify a user.<br />
This inability to edit or destroy a user without authorization<br />
is verified by these automated tests:
<ul>
<li><code>should redirect edit when not logged in</code></li>
<li><code>should redirect edit when logged in as wrong user</code></li>
<li><code>should redirect update when not logged in</code></li>
<li><code>should redirect update when logged in as wrong user</code></li>
<li><code>should redirect destroy when not logged in</code></li>
<li><code>should redirect destroy when logged in as wrong non-admin user</code></li>
</ul>
</li>
</ul>
<p>The <code>additional_rights</code> table, described below, is edited as<br />
part of editing its corresponding project or deleting its<br />
corresponding user, and so does not need to be discussed separately.<br />
No other data can be modified by normal users.</p>
<p>It is also possible to directly modify the underlying database<br />
that records the data.<br />
However, only an administrator with deployment platform access<br />
is authorized to do that, and few people have that privilege.<br />
The deployment platform infrastructure verifies authentication and<br />
authorization.</p>
<p>There is an odd special case involving the repository URL <code>repo_url</code>.<br />
We are trying to counter subtle attacks where<br />
a project tries to claim the good reputation or effort of another project<br />
by constantly switching its <code>repo_url</code> to other projects and/or nonsense.<br />
The underlying problem is that names/identities are hard; the <code>repo_url</code><br />
(when present) is the closest to an &quot;identity&quot; that we have for a project.<br />
We have to allow it to change sometimes (because it sometimes does), but<br />
it should be a rare &quot;sticky&quot; event.<br />
There are various special cases, e.g., you can always set the <code>repo_url</code><br />
if it's nil, the setter is an admin, or if only the scheme is changed.<br />
But otherwise normal users can't change the <code>repo_urls</code> in less than<br />
<code>REPO_URL_CHANGE_DELAY</code> days (a constant set in the projects controller).<br />
Allowing users to change <code>repo_urls</code>, but only<br />
with large delays, reduces the administration effort required.<br />
By doing this, we help protect the integrity of the overall database<br />
from potentially-malicious authorized users.</p>
<h4><a href="#modification-to-official-application-requires-authorization-via-github" aria-hidden="true" class="anchor" id="modification-to-official-application-requires-authorization-via-github"></a>Modification to official application requires authorization via GitHub</h4>
<p>Modifications to the official BadgeApp application require<br />
authorization via GitHub.<br />
We use GitHub for managing the source code and issue tracker; it<br />
has an authentication and authorization system for this purpose.</p>
<h3><a href="#availability" aria-hidden="true" class="anchor" id="availability"></a>Availability</h3>
<p>As with any publicly-accessible website,<br />
we cannot prevent an attacker with significant<br />
resources from temporarily overwhelming the system through<br />
a distributed denial-of-service (DDos) attacks.<br />
So instead, we focus on various kinds of resilience against DDoS attacks,<br />
and use other measures (such as backups) to maximize availability.<br />
Thus, even if the system is taken down temporarily, we expect to be<br />
able to reconstitute it (including its data).</p>
<h4><a href="#cloud--cdn-deployment-allow-quick-scale-up" aria-hidden="true" class="anchor" id="cloud--cdn-deployment-allow-quick-scale-up"></a>Cloud &amp; CDN deployment allow quick scale-up</h4>
<p>We can quickly add more resources if more requests are made.<br />
See the design section &quot;availability through scalability&quot; below<br />
for more about how we handle scaling up.</p>
<h4><a href="#timeout" aria-hidden="true" class="anchor" id="timeout"></a>Timeout</h4>
<p>All user requests have a timeout in production.<br />
That way, the system is not permanently &quot;stuck&quot; on a request.<br />
This is set by setting <code>Rack::Timeout.service_timeout</code><br />
in file <code>config/environments/production.rb</code>.</p>
<h4><a href="#can-return-to-operation-quickly-after-ddos-ended" aria-hidden="true" class="anchor" id="can-return-to-operation-quickly-after-ddos-ended"></a>Can return to operation quickly after DDoS ended</h4>
<p>The system can return to operation quickly after<br />
a DDoS attack has ended.</p>
<h4><a href="#login-disabled-mode" aria-hidden="true" class="anchor" id="login-disabled-mode"></a>Login disabled mode</h4>
<p>We have implemented a &quot;login disabled mode&quot;<br />
(aka <code>BADGEAPP_DENY_LOGIN</code> mode) that we can quickly enable.</p>
<p>This mode is an intentionally degraded mode of operation<br />
that prevents any changes by users (daily statistics<br />
creates are unaffected).<br />
More specifically, if this mode is enabled<br />
then no one can log in to the BadgeApp application,<br />
no one can create a new account (sign up),<br />
and no one can do anything that requires being logged in<br />
(users are always treated as if they are not logged in).</p>
<p>This mode is intended to make some services available<br />
if there is a serious exploitable<br />
security vulnerability that can only be exploited by users who are<br />
logged in or can appear to be logged in.  Unlike <em>completely</em> disabling the<br />
site, this mode allows people to see current information<br />
(such as badge status, project data, and public user data).<br />
This mode is useful because it can stop many attacks, while still providing<br />
some services.</p>
<p>This mode is enabled by setting the<br />
environmental variable <code>BADGEAPP_DENY_LOGIN</code> to a<br />
non-blank value (<code>true</code> is recommended).<br />
Note that application administrators cannot log in, or use their privileges,<br />
when this mode is enabled.<br />
Only hosting site administrators can turn this mode<br />
on or off (since they're the only ones who can set environment variables).</p>
<p>This mode is checked on application startup by<br />
<code>config/initializers/deny_login.rb</code> which sets the boolean variable<br />
<code>Rails.application.config.deny_login</code>.<br />
Its effects can be verified by running<br />
<code>grep -R 'Rails.application.config.deny_login' app/</code>;<br />
they are as follows:</p>
<ul>
<li>Users are never considered logged in, even if they already logged in.<br />
This is enforced in the <code>current_user</code> method in<br />
<code>app/helpers/sessions_helper.rb</code> - this always returns null (not logged in)<br />
when this deny mode is enabled.<br />
This is verified by test<br />
<code>current_user returns nil when deny_login</code>.</li>
<li>Attempts to login are rejected via the <code>create</code> method<br />
of the session controller, per <code>app/controllers/sessions_controller.rb</code>.<br />
Technically this isn't necessary, since being logged in is ignored,<br />
but this rejection will alert users who start trying to log in before<br />
this mode was enabled.<br />
This is verified by test <code>local login fails if deny_login</code>.</li>
<li>Attempts to create a new user account are rejected<br />
via the <code>create</code> method of the user controller, per<br />
<code>app/controllers/users_controller.rb</code>.<br />
We do not want the user database to change while this mode is in effect.<br />
This is verified by test<br />
<code>cannot create local user if login disabled</code></li>
</ul>
<p>Some views are also changed when this view is enabled.<br />
These changes are not security-critical.<br />
Instead, these changes provide users immediate feedback<br />
to help them understand that this special mode has been enabled.</p>
<h4><a href="#multiple-backups" aria-hidden="true" class="anchor" id="multiple-backups"></a>Multiple backups</h4>
<p>We routinely backup the database every day<br />
and retain multiple versions of backups.<br />
That way, if the project data is corrupted, we can restore the<br />
database to a previous state.</p>
<h4><a href="#see-also" aria-hidden="true" class="anchor" id="see-also"></a>See also</h4>
<p>Later in this assurance case we'll note other<br />
capabilities that also aid availability:</p>
<ul>
<li>As noted later in the hardening section, we also have rate limits on<br />
incoming requests, including the number of requests a<br />
client IP address can make in a given period.<br />
This provides a small amount of additional automated protection against<br />
being overwhelmed.</li>
<li>As noted later in the &quot;Recovery plan including backups&quot;,<br />
we have a recovery plan that builds on our multiple backups.</li>
</ul>
<h3><a href="#access-control" aria-hidden="true" class="anchor" id="access-control"></a>Access Control</h3>
<p>Many of the CIA triad requirements address &quot;authorized&quot; users,<br />
and that requires knowing what &quot;authorized&quot; means.<br />
Thus, like nearly all systems, we must address access control,<br />
which we can divide into identification, authentication, and authorization.<br />
Identity, authentication, and authorization are handled in a traditional<br />
manner, as described below.</p>
<h4><a href="#identification" aria-hidden="true" class="anchor" id="identification"></a>Identification</h4>
<p>Normal users must must first identify themselves in one of two ways:<br />
(1) as a GitHub user with their github account name, or<br />
(2) as a custom &quot;local&quot; user with their email address.</p>
<p>The BadgeApp application runs on a deployment platform (Heroku),<br />
which has its own login mechanisms.<br />
Only those few administrators with deployment platform access have<br />
authorization to log in there, and those are protected by the<br />
deployment platform supplier (and thus we do not consider them further here).<br />
The login credentials in these cases are protected.</p>
<h4><a href="#authentication" aria-hidden="true" class="anchor" id="authentication"></a>Authentication</h4>
<p>As with most systems, it's critically<br />
important that authentication work correctly.<br />
Therefore, in this section we'll go into some detail about how<br />
authentication works within the BadgeApp application.</p>
<p>This system implements two kinds of users: local and remote.<br />
Local users log in using a password, but<br />
user passwords are only stored on the server as<br />
iterated salted hashes (using bcrypt).<br />
Remote users use a remote system (we currently only support GitHub)<br />
using the widely-used OAUTH protocol.<br />
At the time the application was written, the recommendation<br />
was to <em>not</em> use libraries like Devise, because they were not mature at<br />
the time. Such libraries have become much more mature, but as of yet<br />
there hasn't been a good reason to change.</p>
<p>The key code for initial authentication is the &quot;sessions&quot; controller file<br />
<code>app/controllers/sessions_controller.rb</code>, and ongoing management is<br />
performed by the application controller and the session helper.<br />
In this section we only consider the login mechanism<br />
built into the BadgeApp.  Heroku has its own login mechanisms, which must<br />
be carefully controlled but are out of scope here.</p>
<h5><a href="#initial-login" aria-hidden="true" class="anchor" id="initial-login"></a>Initial login</h5>
<p>A user who views &quot;/login&quot; will be routed to GET sessions#new, which returns<br />
the login page.  From there:</p>
<ul>
<li>A local user login will POST that information to /login, which is<br />
routed to session#create along with parameters such as session[email]<br />
and session[password].  If the bcrypt'ed hash of the password matches<br />
the stored hash, the user is accepted.<br />
If password doesn't match, the login is rejected.<br />
This is verified with these tests:
<ul>
<li><code>Can login and edit using custom account</code></li>
<li><code>Cannot login with local username and wrong password</code></li>
<li><code>Cannot login with local username and blank password</code></li>
</ul>
</li>
<li>A remote user login (pushing the &quot;log in with GitHub&quot; button) will<br />
invoke GET &quot;/auth/github&quot;.  The application then begin an omniauth<br />
login, by redirecting the user to &quot;<a href="https://github.com/login">https://github.com/login</a>?&quot;<br />
with URL parameters of <code>client_id</code> and <code>return_to</code>.<br />
When the GitHub login completes, then per the omniauth spec there's a<br />
redirect back to our site to /auth/github/callback, which is<br />
routed to session#create along with values such as<br />
the parameter session[provider] set to 'GitHub', which we then check<br />
by using the omniauth-github gem (this is the &quot;callback phase&quot;).<br />
If we confirm that GitHub asserts that the user is authenticated,<br />
then we accept GitHub's ruling for that github user and log them in.<br />
This interaction with GitHub uses <code>GITHUB_KEY</code> and <code>GITHUB_SECRET</code>.<br />
For more information, see the documentation on omniauth-github.<br />
Note that we trust GitHub to verify a GitHub account (as we must).<br />
This is verified as part of the test <code>Has link to GitHub Login</code>.</li>
</ul>
<p>The first thing that session#create does is run <code>counter_fixation</code>;<br />
this counters session fixation attacks<br />
(it also saves the forwarding url, in case we want to return to it).</p>
<p>Local users may choose to &quot;remember me&quot; to automatically re-login on<br />
that specific browser if they use a local account.<br />
This is implemented using a<br />
cryptographically random nonce called <code>remember_token</code> that is<br />
stored in the user's cookie store as a permanent cookie.<br />
It's cryptographically random because it is created by the user model<br />
method <code>self.new_token</code> which calls <code>SecureRandom.urlsafe_base64</code>.<br />
This <code>remember_token</code> acts like a password, which is verified against a<br />
<code>remember_digest</code> value stored in the server<br />
that is an iterated salted hash (using bcrypt).<br />
This &quot;remember me&quot; functionality cannot reveal the user's<br />
original password, and if the server's user database is<br />
compromised an attacker cannot easily determine the nonce used to log in.<br />
The nonce is protected in transit by HTTPS (discussed elsewhere).<br />
The <code>user_id</code> stored by the user is signed by the server.</p>
<p>As with any &quot;remember me&quot; system, this functionality has a<br />
weakness: if the user's system is compromised, others can copy the<br />
<code>remember_token</code> value and then log in as that user using the token<br />
if they use it before it expires.<br />
But this weakness is fundamental to any &quot;remember me&quot;<br />
functionality, and users must opt in to enable &quot;remember me&quot;<br />
(by default users must enter their password on each login,<br />
and the login becomes invalid when the user logs out or when<br />
the user exits the entire browser, because the cookie for login<br />
is only a session cookie).<br />
The &quot;remember me&quot; box was originally implemented<br />
in commit e79decec67.<br />
Note that GitHub users cannot use &quot;remember me&quot; tokens - they can only<br />
authenticate using OAuth. This is enforced both in the user interface<br />
(the &quot;remember me&quot; checkbox only appears in the local login form)<br />
and in the code (the <code>User#remember</code> method raises an <code>ArgumentError</code><br />
if called on a GitHub user, and <code>try_remember_token_login</code> explicitly<br />
rejects GitHub users even if they somehow have a remember cookie).<br />
This separation is important because GitHub authentication requires<br />
a fresh OAuth token, which cannot be stored in the database for<br />
security reasons (OAuth tokens are session-scoped).</p>
<h5><a href="#managing-sessions-after-login" aria-hidden="true" class="anchor" id="managing-sessions-after-login"></a>Managing sessions after login</h5>
<p>A session is created for each user who successfully logs in.<br />
Session data is stored in encrypted cookies managed by Rails.</p>
<p>Session state is managed through a two-tier architecture that<br />
optimizes performance while maintaining security:</p>
<p><strong>Session Extraction and Validation</strong> (<code>setup_authentication_state</code>):<br />
On every request, before any controller action that uses data runs, the<br />
<code>ApplicationController#setup_authentication_state</code> <code>before_action</code><br />
extracts and validates authentication state from the encrypted session cookie.<br />
This is the <em>only</em> place where session authentication data is extracted.<br />
It performs these critical security checks:</p>
<ol>
<li>Extracts <code>session[:user_id]</code> and <code>session[:time_last_used]</code> from the<br />
encrypted session cookie (decrypting the cookie only once per request)</li>
<li>Validates the session timestamp - if missing or older than 48 hours<br />
(<code>SESSION_TTL</code>), the session is rejected and reset to prevent session<br />
tampering and enforce timeout</li>
<li>If no valid session exists, attempts to restore login using a<br />
remember token cookie (only for local users, as explained above)</li>
<li>Stores the validated authentication state in instance variables:<br />
<code>@session_user_id</code>, <code>@session_timestamp</code>, <code>@session_user_token</code><br />
(GitHub OAuth token, if applicable), and <code>@session_github_name</code></li>
</ol>
<p>The <code>setup_authentication_state</code> method does<br />
<em>not</em> query the database in the normal case,<br />
making authentication checks extremely fast.<br />
This is adequate for showing the GUI for logged-in users, as merely<br />
showing the GUI doesn't give the user any special abilities if the<br />
user account has since been deleted.</p>
<p>The <code>SessionsHelper#current_user</code> method lazily loads the full User record<br />
from the database only when needed (e.g., to check admin status or<br />
verify the user still exists).<br />
It uses <code>@session_user_id</code> previously set by <code>setup_authentication_state</code> as<br />
input and memoizes the result in <code>@current_user</code>.<br />
This lazy-loading approach means:</p>
<ul>
<li>Simple authentication checks like <code>logged_in?</code> (which just checks<br />
<code>@session_user_id.present?</code>) require no database access</li>
<li>Authorization checks that need user details automatically trigger<br />
only one database query, cached for the request duration</li>
<li>Recently-deleted users are properly handled:<br />
their session claims they're logged in,<br />
but <code>current_user</code> returns nil, and authorization checks fail safely</li>
</ul>
<p><strong>Session Timeout and Refresh</strong>:<br />
After each controller action, the <code>update_session_timestamp</code> after_action<br />
checks if the session timestamp is older than 1 hour (<code>RESET_SESSION_TIMER</code>).<br />
If so, it updates both <code>session[:time_last_used]</code> and <code>@session_timestamp</code><br />
to the current time. This dual update ensures:</p>
<ol>
<li>The session cookie gets a fresh timestamp (preventing timeout)</li>
<li>The cached instance variable stays synchronized (preventing stale data<br />
from being used later in the same request)</li>
</ol>
<p>The 1-hour threshold balances security (regular timestamp updates) with<br />
performance (avoiding constant session cookie encryption on every request).</p>
<p>This architecture provides:</p>
<ul>
<li><strong>Session tampering prevention</strong>: Sessions without timestamps or with<br />
expired timestamps are rejected.</li>
<li><strong>Automatic timeout</strong>: Inactive sessions expire after 48 hours, limiting<br />
the window for session hijacking</li>
<li><strong>Minimal database load</strong>: Authentication state is cached in instance<br />
variables, and database queries only occur when authorization checks<br />
require current user data</li>
<li><strong>Defense in depth</strong>: Multiple layers (session validation, timestamp checks,<br />
user existence verification) must all succeed for authorization to proceed</li>
<li><strong>Clear separation</strong>: Session validation in the controller is separated from<br />
user lookup in the helper, making the code easier to audit and test</li>
</ul>
<h4><a href="#authorization" aria-hidden="true" class="anchor" id="authorization"></a>Authorization</h4>
<p>Users who have not authenticated themselves can only perform<br />
actions allowed to anyone in the public (e.g., view the home page,<br />
view the list of projects, and view the information about each project).<br />
Once users are authenticated they are authorized to perform certain<br />
additional actions depending on their permissions.</p>
<p>The permissions system is intentionally simple.<br />
As noted above,<br />
every user has an account, either a 'local' account or an external<br />
system account (currently we support GitHub as an external account).<br />
A user with role='admin' is an administrator;<br />
few users are administrators, and only those with direct platform (Heroku)<br />
access can set a user to be an administrator.</p>
<p>Anyone can create a normal user account.<br />
Only that user, or an administrator, can edit or delete a user account.</p>
<p>A user can create as many project entries as desired.<br />
Each project entry gets a new unique project id and is<br />
owned by the user who created the project entry.</p>
<p>There are two kinds of rights over project data:<br />
&quot;control&quot; rights and &quot;edit&quot; rights.</p>
<p>&quot;Control&quot; rights mean you can delete the project AND<br />
change who else is allowed to edit (they control their projects'<br />
entry in the <code>additional_rights</code> table). Anyone with control rights<br />
also has edit rights.  The project owner has control<br />
rights to the projects they own,<br />
and admins have control rights over all projects.<br />
This is determined by the method <code>can_control?</code>.</p>
<p>&quot;Edit&quot; rights mean you can edit the project entry. If you have<br />
control rights over a project you also have edit rights.<br />
In addition, fellow committers on GitHub for that project (if on GitHub),<br />
and users in the <code>additional_rights</code> table<br />
who have their <code>user_id</code> listed for that project, get edit rights<br />
for that project.</p>
<p>If a GitHub user tries to edit a project on GitHub, and the user<br />
is not the badge owner, we permit edits in the following cases:</p>
<ol>
<li>The user is the repo owner. We can tell this because their login nickname<br />
matches the user name of the repo owner on GitHub.</li>
<li>GitHub reports that the user is allowed to edit the project.<br />
We determine this (as of 2020-04-16) by using the GitHub repos API<br />
<a href="https://api.github.com/:owner/:repo">https://api.github.com/:owner/:repo</a> and checking the field &quot;permissions&quot;.<br />
We consider a user with <code>push</code> permissions an editor of the project,<br />
and thus someone who can edit the badge entry.<br />
This request only lists the permissions for that one repo, so this works<br />
relatively quickly even if a GitHub user has many permissions.<br />
(At one time we asked for a list of all user permissions, but that<br />
times out if a user has many permissions, and we didn't really want<br />
most of that data anyway.)<br />
We trust GitHub to provide correct data if it provides this data,<br />
but asking for this data causes delay (as we wait for this response)<br />
and there's a small risk that GitHub might stop reporting this data<br />
(it is not well-documented).  We've countered those concerns with other<br />
steps. In particular, in most cases editors are either badge owners and/or<br />
the repo owner, and we check that first. This is faster, and in most<br />
cases editing will keep working even if GitHub stops reporting this<br />
data. In addition, the <code>additional_rights</code> table can always provide<br />
this functionality no matter what.</li>
</ol>
<p>The <code>additional_rights</code> table adds support for groups so that they can<br />
edit project entries in arbitrary cases<br />
(e.g., when the project is not on GitHub or a user<br />
is not on GitHub).<br />
This is determined by the method <code>can_edit?</code>.</p>
<p>This means that<br />
a project entry can only be edited (and deleted) by the entry creator,<br />
an administrator, by others who can prove that they<br />
can edit that GitHub repository (if it is on GitHub), and by those<br />
authorized to edit via the <code>additional_rights</code> table.<br />
Anyone can see the project entry results once they are saved.</p>
<p>We expressly include tests in our test suite<br />
of our authorization system<br />
to check that accounts cannot perform actions they are not authorized<br />
to perform (e.g., edit a project that they do not have edit rights to,<br />
or delete a project they do not control).<br />
It's important to test that certain actions that <em>must</em> fail for<br />
security reasons do indeed fail.<br />
For more, see the earlier section justifying the claim that<br />
&quot;Data modification requires authorization&quot;.</p>
<h3><a href="#assets--threat-actors-identified--addressed" aria-hidden="true" class="anchor" id="assets--threat-actors-identified--addressed"></a>Assets &amp; threat actors identified &amp; addressed</h3>
<h4><a href="#assets" aria-hidden="true" class="anchor" id="assets"></a>Assets</h4>
<p>As should be clear from the basic requirements above, our assets are:</p>
<ul>
<li>User passwords, especially for confidentiality.<br />
Unencrypted user passwords are the most critical<br />
to protect. As noted above, we protect these with bcrypt;<br />
we never store user passwords in an unencrypted or recoverable form.</li>
<li>The &quot;remember me&quot; nonce if a user requests it - we protect<br />
its confidentiality on the server side.</li>
<li>User email addresses, especially for confidentiality.</li>
<li>Project data, primarily for integrity and availability.<br />
We back these up to support availability.</li>
</ul>
<h4><a href="#threat-agents" aria-hidden="true" class="anchor" id="threat-agents"></a>Threat Agents</h4>
<p>We have few insiders, and they are fully trusted to <em>not</em><br />
perform intentionally-hostile actions.</p>
<p>Thus, the threat agents we're primarily concerned about are outsiders,<br />
and the most concerning ones fit in one of these categories:</p>
<ul>
<li>people who enjoy taking over systems (without monetary benefit)</li>
<li>criminal organizations who want to take emails and/or passwords<br />
as a way to take over others' accounts (to break confidentiality).<br />
Note that our one-way iterated salted hashes counter easy access<br />
to passwords, so the most sensitive data is more difficult to obtain.</li>
<li>criminal organizations who want destroy all our data and hold it for<br />
ransom (i.e., &quot;ransomware&quot; organizations).  Note that our backups<br />
help counter this.</li>
</ul>
<p>Criminal organizations may try to DDoS us for money, but there's no<br />
strong reason for us to pay the extortion fee.<br />
We expect that people will be willing to come back to the site later<br />
if it's down, and we have scalability countermeasures to reduce their<br />
effectiveness.  If the attack is ongoing, several of the services we use<br />
would have a financial incentive to help us counter the attacks.<br />
This makes the attacks themselves less likely<br />
(since there would be no financial benefit to them).</p>
<p>Like many commercial sites,<br />
we do not have the (substantial) resources necessary<br />
to counter a state actor who decided to directly attack our site.<br />
However, there's no reason a state actor would directly attack the site<br />
(we don't store anything that valuable), so while many are very capable,<br />
we do not expect them to be a threat to this site.</p>
<h3><a href="#other-notes-on-security-requirements" aria-hidden="true" class="anchor" id="other-notes-on-security-requirements"></a>Other Notes on Security Requirements</h3>
<p>Here are a few other notes about the security requirements.</p>
<p>It is difficult to implement truly secure software.<br />
One challenge is that BadgeApp must accept, store, and retrieve data from<br />
untrusted (non-admin) users.<br />
In addition, BadgeApp must also go out<br />
to untrusted websites with untrusted contents,<br />
using URLs provided by untrusted users,<br />
to gather data about those projects (so it can automatically fill in data).<br />
By &quot;untrusted&quot; we mean sites that might attempt to attack BadgeApp, e.g.,<br />
by providing malicious data or by being unresponsive.<br />
We have taken a number of steps to reduce the likelihood<br />
of vulnerabilities, and to reduce the impact of vulnerabilities<br />
where they exist.<br />
In particular, retrieval of external information is subject to a timeout,<br />
we use Ruby (a memory-safe language),<br />
and exceptions halt automated processing for that entry (which merely<br />
disables automated data gathering for that entry).</p>
<p>Here we have identified the key security requirements and why we believe<br />
they've been met overall.  However, there is always the possibility that<br />
a mistake could lead to failure to meet these requirements.<br />
It is not possible to eliminate all possible risks; instead,<br />
we focus on <em>managing</em> risks.<br />
We manage our security risks by<br />
implementing security in our software life cycle processes.<br />
We also protect our development environment and choose people<br />
who will help support this.<br />
The following sections describe how we've managed our security-related risks.</p>
<h2><a href="#security-in-design" aria-hidden="true" class="anchor" id="security-in-design"></a>Security in Design</h2>
<p>We emphasize security in the architectural design.</p>
<p>We first present a brief summary of the high-level design,<br />
followed by the results of threat modeling that are based on the design<br />
(this entire document is the result of threat modeling in the<br />
broader sense).<br />
The then discuss approaches we are using in the design<br />
to improve security:<br />
using a simple design,<br />
applying secure design principles,<br />
limiting memory-unsafe language use, and<br />
increasing availability through scalability.</p>
<p>The design, including the security-related items identified here,<br />
were developed through our<br />
design process, which merges<br />
three related processes in ISO/IEC/IEEE 12207<br />
(architecture definition process, design definition process, and<br />
system analysis process).<br />
In particular, the STRIDE analysis results (below) are the primary output<br />
of our system analysis process.</p>
<h3><a href="#high-level-design" aria-hidden="true" class="anchor" id="high-level-design"></a>High-level Design</h3>
<p>The following figure shows a high-level design of the implementation:</p>
<p><img src="./design.png" alt="Design" /></p>
<p>See the <a href="./implementation.html">implementation</a> file to<br />
see a more detailed discussion of the software design.</p>
<h3><a href="#threat-model-focusing-on-design" aria-hidden="true" class="anchor" id="threat-model-focusing-on-design"></a>Threat model focusing on design</h3>
<p>There are many approaches for threat (attack) modeling, e.g., a<br />
focus on attackers, assets, or the design.<br />
We have already discussed attackers and assets; here we focus on the design.</p>
<p>Here we have decided to apply a simplified version of<br />
Microsoft's STRIDE approach for threat modeling.<br />
As explained in<br />
<a href="https://msdn.microsoft.com/en-us/library/ee823878%28v=cs.20%29.aspx">The STRIDE Threat Model</a>, each major design component is examined for:</p>
<ul>
<li>Spoofing identity. An example of identity spoofing is illegally accessing and then using another user's authentication information, such as username and password.</li>
<li>Tampering with data. Data tampering involves the malicious modification of data. Examples include unauthorized changes made to persistent data, such as that held in a database, and the alteration of data as it flows between two computers over an open network, such as the Internet.</li>
<li>Repudiation. Repudiation threats are associated with users who deny performing an action without other parties having any way to prove otherwise - for example, a user performs an illegal operation in a system that lacks the ability to trace the prohibited operations. Non-repudiation refers to the ability of a system to counter repudiation threats. For example, a user who purchases an item might have to sign for the item upon receipt. The vendor can then use the signed receipt as evidence that the user did receive the package.</li>
<li>Information disclosure. Information disclosure threats involve the exposure of information to individuals who are not supposed to have access to it-for example, the ability of users to read a file that they were not granted access to, or the ability of an intruder to read data in transit between two computers.</li>
<li>Denial of service. Denial of service (DoS) attacks deny service to valid users-for example, by making a Web server temporarily unavailable or unusable. You must protect against certain types of DoS threats simply to improve system availability and reliability.</li>
<li>Elevation of privilege. In this type of threat, an unprivileged user gains privileged access and thereby has sufficient access to compromise or destroy the entire system. Elevation of privilege threats include those situations in which an attacker has effectively penetrated all system defenses and become part of the trusted system itself, a dangerous situation indeed.</li>
</ul>
<p>The diagram shown earlier is not a data flow diagram<br />
(DFD), but it can be interpreted as one by interpreting<br />
the arrows as two-way data flows.<br />
This is frankly too detailed for such a simple system, so we will<br />
group rectangles together into a smaller set of processes as shown below.</p>
<h4><a href="#web-server-web-app-interface-and-router" aria-hidden="true" class="anchor" id="web-server-web-app-interface-and-router"></a>Web server, Web App Interface, and Router</h4>
<p>The web server and webapp interface accept untrusted data and deliver<br />
it to the appropriate controller.</p>
<ul>
<li>Spoofing identity. N/A, identity is irrelevant because it's untrusted.</li>
<li>Tampering with data. Data is only accepted by the web server via HTTPS.</li>
<li>Repudiation. N/A.</li>
<li>Information disclosure. These simply deliver untrusted data to components<br />
we trust to handle it properly.</li>
<li>Denial of service. We use scalability, caching, a CDN,<br />
and rapid recovery to help deal with denial of service attacks.<br />
Large denial of service attacks are hard to counter, and we don't claim<br />
to be able to prevent them.</li>
<li>Elevation of privilege. By itself these components provide no privilege.</li>
</ul>
<h4><a href="#controllers-models-views" aria-hidden="true" class="anchor" id="controllers-models-views"></a>Controllers, Models, Views</h4>
<ul>
<li>Spoofing identity. Identities are authenticated before they are used.<br />
Session values are sent back to the user, but stored in an encrypted<br />
container and only the server has the encryption key.</li>
<li>Tampering with data.<br />
User authorization is checked before changes are permitted.</li>
<li>Repudiation. N/A.</li>
<li>Information disclosure.  Sensitive data (passwords and email addresses)<br />
is not displayed in any view unless the user is an authorized admin.<br />
Our contributing documentation expressly forbids storing email addresses<br />
in the Rails cache; that way, if we accidentally display the wrong<br />
cache, no email address will be revealed.</li>
<li>Denial of service. See earlier comments on DoS.</li>
<li>Elevation of privilege.  These are written in a memory-safe language,<br />
and written defensively (since normal users are untrusted).<br />
There's no known way to use an existing<br />
privilege to gain more privileges.<br />
In addition, the application has no built-in mechanism<br />
for turning normal users into administrators; this must be done using<br />
the SQL interface that is only available to those who have admin rights<br />
to access the SQL database.  That's no guarantee of invulnerability,<br />
but it means that there's no pre-existing code that can be triggered<br />
to cause the change.</li>
</ul>
<h4><a href="#dbms" aria-hidden="true" class="anchor" id="dbms"></a>DBMS</h4>
<p>There is no direct access for normal users to the DBMS;<br />
in production, access requires special Heroku keys.</p>
<p>The DBMS does not know which user the BadgeApp<br />
is operating on behalf of, and does not have separate privileges.<br />
However, the BadgeApp uses ActiveRecord and parameterized statements,<br />
making it unlikely that an attacker can use SQL injections to<br />
insert malicious queries.</p>
<ul>
<li>Spoofing identity. N/A, the database doesn't track identities.</li>
<li>Tampering with data. The BadgeApp is trusted to make correct requests.</li>
<li>Repudiation. N/A.</li>
<li>Information disclosure.  The BadgeApp is trusted to make correct requests.</li>
<li>Denial of service. See earlier comments on DoS.</li>
<li>Elevation of privilege.  N/A, the DBMS doesn't separate privileges.</li>
</ul>
<h4><a href="#chief-and-detectives" aria-hidden="true" class="anchor" id="chief-and-detectives"></a>Chief and Detectives</h4>
<ul>
<li>Spoofing identity. N/A, these simply collect data.</li>
<li>Tampering with data. These use HTTPS when provided HTTPS URLs.</li>
<li>Repudiation. N/A.</li>
<li>Information disclosure.  These simply retrieve and summarize<br />
information that is publicly available, using URLs provided by users.</li>
<li>Denial of service.  Timeouts are in place so that if the project<br />
isn't responsive, eventually the system automatically recovers.</li>
<li>Elevation of privilege.  These are written in a memory-safe language,<br />
and written defensively (since the project sites are untrusted).</li>
</ul>
<h4><a href="#admin-cli" aria-hidden="true" class="anchor" id="admin-cli"></a>Admin CLI</h4>
<p>There is a command line interface (CLI) for admins.<br />
This is the Heroku CLI.<br />
Admins must use their unique credentials to log in.<br />
<a href="https://github.com/heroku/cli/blob/master/http.go">The channel between the admin and the Heroku site is encrypted using TLS</a>.</p>
<ul>
<li>Spoofing identity. Every admin has a unique credential.</li>
<li>Tampering with data. The communication channel is encrypted.</li>
<li>Repudiation. Admins have unique credentials.</li>
<li>Information disclosure.  The channel is encrypted in motion.</li>
<li>Denial of service.  Heroku has a financial incentive to keep this<br />
available, and takes steps to do so.</li>
<li>Elevation of privilege.  N/A; anyone allowed to use this is privileged.</li>
</ul>
<h4><a href="#translation-service-and-i18n-text" aria-hidden="true" class="anchor" id="translation-service-and-i18n-text"></a>Translation service and I18n text</h4>
<p>This software is internationalized.</p>
<p>All text used for display is in the directory &quot;config/locales&quot;; on the figure<br />
this is shown as I18n (internationalized) text.<br />
The source text specific to the application is in English<br />
in file config/locales/en.yml.<br />
The &quot;rake translation:sync&quot; command, which is executed within the<br />
<em>development</em> environment, transmits the current version of en.yml<br />
to the site translation.io, and loads the current text from translation.io into<br />
the various config/locales files.<br />
Only authorized translators are given edit rights to translations on<br />
translation.io.</p>
<p>We consider translation.io and our translators as trusted.<br />
That said, we impose a variety of security safeguards as if they were not<br />
trusted.  That way, if something happens (e.g., someone's account is<br />
subverted), then the damage that can be done is limited.</p>
<p>Here are the key security safeguards:</p>
<ul>
<li>During &quot;translation:sync&quot; synchronization,<br />
the &quot;en.yml&quot; file downloaded from translation.io is erased, and<br />
the original &quot;en.yml&quot; is restored.  Thus, translation.io <em>cannot</em> modify<br />
the English source text.</li>
<li>After synchronization, and on every test run (including deployment to a tier),<br />
<em>every</em> text segment (including English) is checked, including to<br />
ensure that <em>only</em> an allowlisted set of HTML tags<br />
and attributes (at most) are<br />
included in every text.  The tests will fail, and the system will not be<br />
deployed, if any other tags or attributes are used.<br />
This set does not include dangerous tags such as &lt;script&gt;.<br />
The test details are in <code>test/models/translations_test.rb</code>.<br />
Thus, while a translation can be wrong or be defaced,<br />
what it can include in the HTML (and thus attack users) is very limited.<br />
Although not relevant to security, it's worth noting that these tests<br />
also check for many errors in translation.  For example, only Latin<br />
lowercase letters are allowed after &quot;&lt;&quot; and &quot;&lt;/&quot;; these protect<br />
against following these sequences with whitespace or a Cyrillic &quot;a&quot;.</li>
<li>Synchronization simply transfers the updated translations to the<br />
directory config/locales.  This is then reviewed by a committer before<br />
committing, and goes through tiers as usual.</li>
</ul>
<p>We don't want the text defaced, and take a number of steps to prevent it.<br />
That said, what's more important is ensuring that defaced text is unlikely<br />
to turn into an attack on our users, so we take <em>extra</em> cautions<br />
to prevent that.</p>
<p>Given these safeguards, here is how we deal with STRIDE:</p>
<ul>
<li>Spoofing identity. Every translator has a unique credential.</li>
<li>Tampering with data. Translators other than admins are only given edit<br />
rights for a particular locale.  The damage is limited, because<br />
the text must pass through an HTML sanitizer.</li>
<li>Repudiation. Those authorized on translation.io have unique credentials.</li>
<li>Information disclosure.  The channel is encrypted in motion, and in<br />
any case other than passwords this is all public information.</li>
<li>Denial of service.  Translation.io has a financial incentive to keep its<br />
service available, and takes steps to do so.<br />
At run-time the system uses its internal text copy, so if<br />
translation.io stops working for a while, our site can continue working.<br />
If it stayed down, we could switch to another service or do it ourselves.</li>
<li>Elevation of privilege.  A translator cannot edit the source text files<br />
by this mechanism.  Sanitization checks limit the damage that can be done.</li>
</ul>
<h3><a href="#simple-design" aria-hidden="true" class="anchor" id="simple-design"></a><a name="simple-design"></a>Simple design</h3>
<p>This web application has a simple design.<br />
It is a standard Ruby on Rails design with models, views, and controllers.<br />
In production it is accessed via a web server (Puma) and<br />
builds on a relational database database system (PostgreSQL).<br />
The software is multi-process and is intended to be multi-threaded<br />
(see the <a href="../CONTRIBUTING.html">CONTRIBUTING.md</a> file for more about this).<br />
The database system itself is trusted, and the database managed<br />
by the database system is not directly accessible by untrusted users.<br />
The application runs on Linux kernel and uses some standard operating system<br />
facilities and libraries (e.g., to provide TLS).<br />
All interaction between the users and the web application go over<br />
an encrypted channel using TLS.<br />
There is some JavaScript served to the client,<br />
but no security decisions depend on code that runs on the client.</p>
<p>The custom code has been kept as small as possible, in particular, we've<br />
tried to keep it DRY (don't repeat yourself).</p>
<p>From a user's point of view,<br />
users potentially create an id, then log in and enter data<br />
about projects (as new or updated data).<br />
Users can log in using a local account or by using their GitHub account.<br />
Non-admin users are not trusted.<br />
The entry of project data (and potentially periodically) triggers<br />
an evaluation of data about the project, which automatically fills in<br />
data about the project.<br />
Projects that meet certain criteria earn a badge, which is displayed<br />
by requesting a specific URL.<br />
A &quot;Chief&quot; class and &quot;Detective&quot; classes attempt to get data about a project<br />
and analyze that data; this project data is also untrusted<br />
(in particular, filenames, file contents, issue tracker information and<br />
contents, etc., are all untrusted).</p>
<h3><a href="#secure-design-principles" aria-hidden="true" class="anchor" id="secure-design-principles"></a>Secure design principles</h3>
<p>Applying various secure design principles helps us avoid<br />
security problems in the first place.<br />
The most widely-used list of security design principles, and<br />
one we build on, is the list developed by<br />
<a href="http://web.mit.edu/Saltzer/www/publications/protection/">Saltzer and Schroeder</a>.</p>
<p>Here are a number of secure design principles and how we follow them,<br />
including all 8 principles from<br />
<a href="http://web.mit.edu/Saltzer/www/publications/protection/">Saltzer and Schroeder</a>:</p>
<ul>
<li>Economy of mechanism (keep the design as simple and small as practical,<br />
e.g., by adopting sweeping simplifications).<br />
We discuss this in more detail in the section<br />
&quot;<a href="#simple-design">simple design</a>&quot;.</li>
<li>Fail-safe defaults (access decisions should deny by default):<br />
Access decisions are deny by default.</li>
<li>Complete mediation (every access that might be limited must be<br />
checked for authority and be non-bypassable):<br />
Every access that might be limited is checked for authority and<br />
non-bypassable.  Security checks are in the controllers, not the router,<br />
because multiple routes can lead to the same controller<br />
(this is per Rails security guidelines).<br />
When entering data, JavaScript code on the client shows whether or not<br />
the badge has been achieved, but the client-side code is <em>not</em> the<br />
final authority (it's merely a convenience).  The final arbiter of<br />
badge acceptance is server-side code, which is not bypassable.</li>
<li>Open design (security mechanisms should not depend on attacker<br />
ignorance of its design, but instead on more easily protected and<br />
changed information like keys and passwords):<br />
The entire program is open source software and subject to inspection.<br />
Keys are kept in separate files not included in the public repository.</li>
<li>Separation of privilege (multi-factor authentication,<br />
such as requiring both a password and a hardware token,<br />
is stronger than single-factor authentication):<br />
We don't use multi-factor authentication because the risks from compromise<br />
are smaller compared to many other systems<br />
(it's almost entirely public data, and failures generally can be recovered<br />
through backups).</li>
<li>Least privilege (processes should operate with the<br />
least privilege necessary): The application runs as a normal user,<br />
not a privileged user like &quot;root&quot;.  It must have read/write access to<br />
its database, so it has that privilege.</li>
<li>Least common mechanism (the design should minimize the mechanisms<br />
common to more than one user and depended on by all users,<br />
e.g., directories for temporary files):<br />
No shared temporary directory is used.  Each time a new request is made,<br />
new objects are instantiated; this makes the program generally thread-safe<br />
as well as minimizing mechanisms common to more than one user.<br />
The database is shared, but each table row has access control implemented<br />
which limits sharing to those authorized to share.</li>
<li>Psychological acceptability<br />
(the human interface must be designed for ease of use,<br />
designing for &quot;least astonishment&quot; can help):<br />
The application presents a simple login and &quot;fill in the form&quot;<br />
interface, so it should be acceptable.</li>
<li>Limited attack surface (the attack surface, the set of the different<br />
points where an attacker can try to enter or extract data, should be limited):<br />
The application has a limited attack surface.<br />
As with all Ruby on Rails applications, all access must go through the<br />
router to the controllers; the controllers then check for access permission.<br />
There are few routes, and few controller methods are publicly accessible.<br />
The underlying database is configured to <em>not</em> be publicly accessible.<br />
Many of the operations use numeric ids (e.g., which project), which are<br />
simply numbers (limiting the opportunity for attack because numbers are<br />
trivial to validate).</li>
<li>Input validation with allowlists<br />
(inputs should typically be checked to determine if they are valid<br />
before they are accepted; this validation should use allowlists<br />
(which only accept known-good values),<br />
not denylists (which attempt to list known-bad values)):<br />
In data provided directly to the web application,<br />
input validation is done with allowlists through controllers and models.<br />
Parameters are first checked in the controllers using the Ruby on Rails<br />
&quot;strong parameter&quot; mechanism, which ensures that only an allowlisted set<br />
of parameters are accepted at all.<br />
Once the parameters are accepted, Ruby on Rails'<br />
<a href="http://guides.rubyonrails.org/active_record_validations.html">active record validations</a><br />
are used.<br />
All project parameters are checked by the model, in particular,<br />
status values (the key values used for badges) are checked against<br />
an allowlist of values allowed for that criterion.<br />
There are a number of freetext fields (name, license, and the<br />
justifications); since they are freetext these are the hardest<br />
to allowlist.<br />
That said, we even impose restrictions on freetext, in particular,<br />
they must be valid UTF-8, they must not include control characters<br />
(other than <span data-escaped-char>\</span>n and <span data-escaped-char>\</span>r), and they have maximum lengths.<br />
These checks by themselves cannot counter certain attacks;<br />
see the text on security in implementation for the discussion on<br />
how this application counters SQL injection, XSS, and CSRF attacks.<br />
URLs are also limited by length and an allowlisted regex, which counters<br />
some kinds of attacks.<br />
When project data (new or edited) is provided, all proposed status values<br />
are checked to ensure they are one of the legal criteria values for<br />
that criterion (Met, Unmet, ?, or N/A depending on the criterion).<br />
Once project data is received, the application tries to get some<br />
values from the project itself; this data may be malevolent, but the<br />
application is just looking for the presence or absence of certain<br />
data patterns, and never executes data from the project.</li>
</ul>
<h3><a href="#availability-through-scalability" aria-hidden="true" class="anchor" id="availability-through-scalability"></a>Availability through scalability</h3>
<p>Availability is, as always, especially challenging.<br />
Our primary approach is to ensure that the design scales.</p>
<p>As a Ruby on Rails application, it is designed so each request can<br />
be processed separately on separate processes.<br />
We use the 'puma' web server to serve multiple processes<br />
(so attackers must have many multiple simultaneous requests to keep<br />
them all busy),<br />
and timeouts are used (once a request times out, the process is<br />
automatically killed and the server can process a new request).<br />
The system is designed to be easily scalable (just add more worker<br />
processes), so we can quickly purchase additional computing resources<br />
to handle requests if needed.</p>
<p>The system is currently deployed to Heroku, which imposes a hard<br />
time limit for each request; thus, if a request gets stuck<br />
(say during autofill by a malevolent actor who responds slowly),<br />
eventually the timeout will cause the response to stop and the<br />
system will become ready for another request.</p>
<p>We use a Content Delivery Network (CDN), specifically Fastly,<br />
to provide cached values of badges.<br />
These are the most resource-intense kind of request, simply because<br />
they happen so often.<br />
As long as the CDN is up, even if the application crashes the<br />
then-current data will stay available until the system recovers.</p>
<p>The system is configured so all requests go through the CDN (Fastly),<br />
then through Heroku; each provides us with some DDoS protections.<br />
If the system starts up with Fastly configured, then the software<br />
loads the set of valid Fastly IP addresses, and rejects any requests<br />
from other IPs.  This prevents &quot;cloud piercing&quot;.<br />
This does use the value of the header X-Forwarded-For, which could<br />
be provided by an attacker, but Heroku guarantees a particular order<br />
so we only retrieve the value that we can trust (through Heroku).<br />
This has been verified to work, because all of the following are rejected:</p>
<pre style="background-color:#2b303b;"><code><span style="color:#c0c5ce;">curl https://master-bestpractices.herokuapp.com/
</span><span style="color:#c0c5ce;">curl -H &quot;X-Forwarded-For: 23.235.32.1&quot; \
</span><span style="color:#c0c5ce;">     https://master-bestpractices.herokuapp.com/
</span><span style="color:#c0c5ce;">curl -H &quot;X-Forwarded-For: 23.235.32.1,23.235.32.1&quot; \
</span><span style="color:#c0c5ce;">     https://master-bestpractices.herokuapp.com/
</span></code></pre>
<p><a href="https://cpdos.org/">Systems that use CDNs can be vulnerable to the Cache Poisoned Denial of Service (CPDoS) family of vulnerabilities</a>.<br />
We do use a CDN (Fastly), but our brief research<br />
suggests that this site is not vulnerable to CPDoS.<br />
The resarchers of CPDoS posted what they found vulnerable, and their<br />
&quot;Fastly&quot; column has empty rows for &quot;Heroku&quot; and &quot;Rails&quot;.<br />
Unfortunately they don't directly list Puma (our webserver),<br />
but Puma is the recommended webserver by Heroku for Rails and<br />
is the usual choice.</p>
<p>The system implements a variety of server-side caches, in particular,<br />
it widely uses fragment caching.  This is primarily to improve performance,<br />
but it also helps with availability against a DDoS, because<br />
once a result has been cached it requires very little effort to<br />
serve the same information again.</p>
<p>A determined attacker with significant resources could disable the<br />
system through a distributed denial-of-service (DDoS) attack.<br />
However, this site doesn't have any particular political agenda,<br />
and taking it down is unlikely to provide monetary gain.<br />
Thus, this site doesn't seem as likely a target for a long-term DDoS<br />
attack, and there is not much else we can do to counter DDoS<br />
by an attacker with significant resources without having<br />
significant resources ourselves.</p>
<h3><a href="#memory-safe-languages" aria-hidden="true" class="anchor" id="memory-safe-languages"></a>Memory-safe languages</h3>
<p>All the code we have written (aka the custom code)<br />
is written in memory-safe languages<br />
(Ruby and JavaScript), so the vulnerabilities of memory-unsafe<br />
languages (such as C and C++) cannot occur in the custom code.<br />
This also applies to most of the code in the directly depended libraries.</p>
<p>Some lower-level reused components (e.g., the operating system kernel,<br />
database management system, encryption library, and some of the Ruby gems)<br />
do have C/C++, but these are widely used components where we have<br />
good reason to believe that developers are directly working to mitigate<br />
the problems from memory-unsafe languages.<br />
See the section below on supply chain (reuse) for more.</p>
<h2><a href="#security-in-implementation" aria-hidden="true" class="anchor" id="security-in-implementation"></a>Security in Implementation</h2>
<p>Most implementation vulnerabilities are due to common types<br />
of implementation errors or common misconfigurations,<br />
so countering them greatly reduces security risks.</p>
<p>To reduce the risk of security vulnerabilities in implementation we<br />
have focused on countering the OWASP Top 10,<br />
both the<br />
<a href="https://www.owasp.org/index.php/Top_10_2013-Top_10">OWASP Top 10 (2013)</a><br />
and<br />
<a href="https://www.owasp.org/index.php/Top_10-2017_Top_10">OWASP Top 10 (2017)</a>.<br />
To counter common misconfigurations, we apply the<br />
<a href="http://guides.rubyonrails.org/security.html">Ruby on Rails Security Guide</a>.<br />
We have also taken steps to harden the application.<br />
Finally, we try to stay vigilant when new kinds of vulnerabilities are<br />
reported that apply to this application, and make adjustments.<br />
Below is how we've done each, in turn.</p>
<h3><a href="#common-implementation-vulnerability-types-countered-owasp-top-10" aria-hidden="true" class="anchor" id="common-implementation-vulnerability-types-countered-owasp-top-10"></a>Common implementation vulnerability types countered (OWASP top 10)</h3>
<p>The OWASP Top 10<br />
(<a href="https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project">details</a>)<br />
represents &quot;a broad consensus about what the most<br />
critical web application security flaws are.&quot;<br />
When this application was originally developed, the current version was<br />
<a href="https://www.owasp.org/index.php/Top_10_2013-Top_10">OWASP Top 10 (2013)</a>.<br />
Since that time the 2017 version, aka<br />
<a href="https://www.owasp.org/index.php/Top_10-2017_Top_10">OWASP Top 10 (2017)</a>,<br />
has become available.<br />
We address all of the issues identified in both lists.<br />
By ensuring that we address all of them,<br />
we address all of the most critical and common flaws for<br />
this we application.</p>
<p>Here are the OWASP top 10<br />
and how we attempt to reduce their risks in BadgeApp.<br />
We list them in order of the ten 2013 items, and then (starting at #11)<br />
list the additional items added since 2013.</p>
<ol>
<li>Injection.<br />
BadgeApp is implemented in Ruby on Rails, which has<br />
built-in protection against SQL injection.  SQL commands are generally<br />
not used directly, instead Rails includes ActiveRecord, which implements an<br />
Object Relational Mapping (ORM) with parameterized commands.<br />
In a few rare cases SQL commands (or fragments of them) are created directly,<br />
but these SQL commands never use unparameterized untrusted inputs.<br />
Any inputs to SQL commands are always parameterized, trusted, or both,<br />
typically using its parametrized mechanisms or similar mechanisms such as<br />
<code>sanitize_sql_like</code>.<br />
Note: Admin user search intentionally does NOT escape LIKE wildcards<br />
(% and _) to support GDPR compliance and legal requests requiring<br />
pattern matching. This is safe because only admin users can access<br />
this functionality (UsersController#search_name).<br />
The shell is not used to download or process file contents (e.g., from<br />
repositories), instead, various Ruby APIs acquire and process it directly.</li>
<li>Broken Authentication and Session Management.<br />
Sessions are created and destroyed through a common<br />
Rails mechanism, including an encrypted and signed cookie authentication<br />
value.</li>
<li>Cross-Site Scripting (XSS).<br />
We use Rails' built-in XSS<br />
countermeasures, in particular, its &quot;safe&quot; HTML mechanisms such<br />
as SafeBuffer.  By default, Rails always applies HTML escapes<br />
on strings displayed through views unless they are marked as safe.<br />
<a href="http://yehudakatz.com/2010/02/01/safebuffers-and-rails-3-0/">SafeBuffers and Rails 3.0</a><br />
discusses this in more detail.<br />
This greatly reduces the risk of mistakes leading to XSS vulnerabilities.<br />
We do process markdown, but markdown processing always checks to ensure<br />
that only specific safe balanced tags are allowed and only specific<br />
safe attributes are allowed, and it marks all external hrefs with<br />
&quot;nofollow ugc noopener noreferrer&quot; to clearly identify such links.<br />
In addition, we use a restrictive Content Security Policy (CSP).<br />
Our CSP, for example, tells web browsers to not execute any JavaScript<br />
included in HTML (JavaScript must be in separate JavaScript files).<br />
This makes limits damage even if an attacker gets something into<br />
the generated HTML.</li>
<li>Insecure Direct Object References.<br />
The only supported direct object references are for publicly available<br />
objects (stylesheets, etc.).<br />
All other requests go through routers and controllers,<br />
which determine what may be accessed.</li>
<li>Security Misconfiguration.<br />
See the section on <a href="#misconfiguration">countering misconfiguration</a>.</li>
<li>Sensitive Data Exposure.<br />
We generally do not store sensitive data; most of the data about projects<br />
is intended to be public.  We do store email addresses, and work to<br />
prevent them from exposure.<br />
The local passwords are potentially the most sensitive; stolen passwords<br />
allow others to masquerade as that user, possibly on other sites<br />
if the user reuses the password on other sites.<br />
Local passwords are encrypted with bcrypt<br />
(this is a well-known iterated salted hash algorithm) using a per-user salt.<br />
We don't store email addresses in the Rails cache, so if even if the<br />
wrong cache is used an email address won't be exposed.<br />
We use HTTPS to establish an encrypted link between the server and users,<br />
to prevent sensitive data (like passwords) from being disclosed in motion.</li>
<li>Missing Function Level Access Control.<br />
The system depends on server-side routers and controllers for<br />
access control.  There is some client-side JavaScript, but no<br />
access control depends on it.</li>
<li>Cross-Site Request Forgery (CSRF or XSRF).<br />
We use the built-in Rails CSRF countermeasure, where csrf tokens<br />
are included in replies and checked on POST inputs.<br />
We also set cookies with SameSite=Lax, which automatically counters<br />
CSRF on supported browsers (such as Chrome).<br />
Our restrictive Content Security Policy (CSP) helps here, too.<br />
For more information, see the<br />
<a href="http://guides.rubyonrails.org/security.html#cross-site-request-forgery-csrf">Ruby on Rails Guide on Security (CSRF)</a> and<br />
<a href="http://api.rubyonrails.org/classes/ActionController/RequestForgeryProtection.html">ActionController request forgery protection</a>.<br />
We can walk through various cases to show that this problem cannot occur<br />
with user Alice, attacker Mallory, and our server:
<ul>
<li>If Alice is not logged in, Alice has no currently-active<br />
privileges for CSRF to exploit.</li>
<li>We'll assume Alice knows that logging into Mallory's<br />
site is not the same as logging into our site,<br />
that our anti-spoofing (&quot;frame busting&quot;) techniques work, and that<br />
TLS (with certificates) works correctly.<br />
Thus, Mallory can't just show a &quot;login here&quot; page that Alice will use.<br />
From here on, we'll assume that Alice is logged in normally through<br />
our website, and that Mallory will try to convince Alice to click<br />
on something on a website controlled by Mallory to create<br />
a CSRF attack (which tries to fool our site through Alice).</li>
<li>If Alice contacts Mallory's website, Alice won't send the session cookie<br />
(so Mallory can't directly spoof Alice's session).<br />
Mallory could create HTML (e.g., hyperlinks and forms) for Alice;<br />
if Alice selects something on Mallory's HTML,<br />
Alice will send a request to our server.<br />
That request from Alice using Mallory's data could be either a GET/HEAD<br />
or something else (such as POST).<br />
So now, let's consider those two sub-cases:
<ul>
<li>GET and HEAD are by design never dangerous requests;<br />
our server merely shows data that Alice is already allowed to see.<br />
So there is no problem in this case.</li>
<li>If the request is something else (such as POST),<br />
then there are two sub-sub-cases:
<ul>
<li>If the request is something else (such as POST),<br />
and Alice is using a browser with SameSite cookie support, Alice<br />
will not send the cookie data - and thus on this request it<br />
would be as if Alice was not logged in (which is safe).</li>
<li>If the request is something else (such as POST),<br />
and Alice is using<br />
a browser without SameSite=Lax support, our server will check to<br />
ensure that the form and cookie data provided by Alice match,<br />
and only allow actions if they match.<br />
In this final case, Mallory never got the cookie data,<br />
so Mallory cannot create a form to match it, foiling Mallory.<br />
Thus, our approach completely counters CSRF.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Using Components with Known Vulnerabilities.<br />
See the maintenance process.</li>
<li>Unvalidated Redirects and Forwards.<br />
Redirects and forwards are used sparingly, and they are validated.</li>
<li>XML External Entities (XXE). This was added in 2017 as &quot;A4&quot;.<br />
Old versions of Rails were vulnerable to some XML external entity<br />
attacks, but the XML parameters parser was removed from core in Rails 4.0,<br />
and we do not re-add that optional feature.<br />
Since we do not accept XML input from untrusted sources, we<br />
cannot be vulnerable.<br />
We do <em>generate</em> XML (for the Atom feed), but that's different.<br />
One area where we may <em>appear</em> to be vulnerable, but we<br />
believe we are not, involves nokogiri, libxml2, and<br />
<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-9318">CVE-2016-9318</a>.<br />
The analysis of Nokogiri is further discussed below in the section<br />
on supply chain.</li>
<li>Insecure Deserialization. This was added in 2017 as &quot;A8&quot;.<br />
This vulnerability would permit remote code execution or<br />
sensitive object manipulation on affected platforms.<br />
The application itself only accepts JSON and HTML fields (POST or GET).<br />
The JSON parser only deserializes to trusted standard objects<br />
which are never executed.<br />
A key component we use, Rails' Action Controller,<br />
<a href="http://guides.rubyonrails.org/action_controller_overview.html#hash-and-array-parameters">does implement hash and array parameters</a>,<br />
but these only generate hashes and arrays - there is no<br />
general deserializer that could lead to an insecurity.</li>
<li>Insufficient Logging and Monitoring. This was added in 2017 as &quot;A10&quot;.<br />
We do logging and monitoring, as discussed elsewhere.</li>
</ol>
<p>Broken Access Control was added in 2017 as &quot;A5&quot;, but it's<br />
really just a merge of the<br />
2013's A4 (Insecure Direct Object References)<br />
2013's A7 (Missing Function Level Access Control), which we've<br />
covered as discussed above.<br />
Thus, we don't list that separately.</p>
<p>We continue to cover the 2013 A8 (Cross-Site Request Forgery (CSRF))<br />
and 2013 A10 (Unvalidated Redirects and Forwards), even thought they are<br />
not listed in the 2017 edition of the OWASP top 10.</p>
<h3><a href="#common-misconfiguration-errors-countered-ruby-on-rails-security-guide" aria-hidden="true" class="anchor" id="common-misconfiguration-errors-countered-ruby-on-rails-security-guide"></a><a name="misconfiguration"></a>Common misconfiguration errors countered: Ruby on Rails Security Guide</h3>
<p>A common security problem with applications is misconfiguration;<br />
here is how we reduce the risks from misconfiguration.</p>
<p>We take a number of steps to counter misconfiguration.<br />
We have strived to enable secure defaults from the start.<br />
We use a number of <a href="#online-checkers">external online checkers</a><br />
to detect common HTTPS misconfiguration problems (see below).<br />
We use Brakeman and CodeQL, which can detect<br />
some misconfigurations in Rails applications.<br />
We invoke static analysis tools (Brakeman and CodeQL) as part of<br />
our continuous integration pipeline.</p>
<p>However, our primary mechanism for countering misconfigurations is by<br />
identifying and apply ing the most-relevant security guide available.</p>
<p>This entire application is built on Ruby on Rails.<br />
The Ruby on Rails developers provide a<br />
<a href="http://guides.rubyonrails.org/security.html">Ruby on Rails Security Guide</a>,<br />
which identifies what they believe are the most important areas to<br />
check for securing such applications.<br />
Since this guide is focused on the infrastructure we use, we think this is<br />
the most important guide for us to focus on.</p>
<p>We apply the entire guide.<br />
Here is a discussion on how we apply the entire guide, per its chapters<br />
as of 2015-12-14:</p>
<ol>
<li><em>Introduction.</em> N/A.</li>
<li><em>Sessions.</em><br />
We use sessions, and use session cookies to store them<br />
because of their wide support and efficiency.<br />
We use the default Rails CookieStore mechanism to store sessions;<br />
it is both simple and much faster than alternatives.<br />
Rails implements an automatic cookie authentication mechanism (using a<br />
secret key referred to as <code>secret_key_base</code>)<br />
to ensure that clients cannot undetectably change<br />
these cookies; a changed value created without being re-authenticated<br />
is thrown away.<br />
Logged-in users have their user id stored in this authenticated cookie<br />
(There is also a <code>session_id</code>, not currently used.)<br />
Session data is intentionally kept small, because of the limited<br />
amount of data available in a cookie.<br />
To counteract session hijacking, we configure the production<br />
environment to always communicate over an encrypted channel using TLS<br />
(see file <code>config/environments/production.rb</code> which sets<br />
<code>config.force_ssl</code> to true).<br />
The design allows users to drop cookies at any time<br />
(at worse they may have to re-login to get another session cookie).<br />
One complaint about Rails' traditional CookieStore is that if someone<br />
gets a copy of a session cookie, they can log in as that user, even<br />
if the cookie is years old and the user logged out.<br />
(e.g., because someone got a backup copied).<br />
Our countermeasure is to time out inactive sessions, by<br />
also storing a <code>time_last_used</code> in the session<br />
cookie (the UTC time the cookie was last used).<br />
Once the time expires, then even if someone else later gets an old<br />
cookie value, it cannot be used to log into the system.</li>
<li><em>Cross-Site Request Forgery (CSRF).</em><br />
We use the standard REST operations with their standard meanings<br />
(GET, POST, etc., with the standard Rails method workaround).<br />
We have a CSRF required security token implemented using<br />
<code>protect_from_forgery</code> built into the application-wide controller<br />
<code>app/controllers/application_controller.rb</code><br />
(we do not use cookies.permanent or similar, a contra-indicator).<br />
Also, we set cookies with SameSite=Lax; this is a useful hardening<br />
countermeasure in browsers that support it.</li>
<li><em>Redirection and Files.</em><br />
The application uses relatively few redirects; those that do involve<br />
the &quot;id&quot;, which only works if it can find the value corresponding to<br />
the id first (which is an allowlist).<br />
File uploads aren't directly supported; the application does<br />
temporarily load some files (as part of autofill), but those filenames<br />
and contents are not directly made available to any other user<br />
(indeed, they're thrown away once autofill completes; caching may<br />
keep them, but that simply allows re-reading of data already acquired).<br />
The files aren't put into a filesystem, so there's no<br />
opportunity for executable code to be put into the filesystem this way.<br />
There is no arbitrary file downloading capability, and private files<br />
(e.g., with keys) are not in the docroot.</li>
<li><em>Intranet and Admin Security.</em><br />
Some users have 'admin' privileges, but these additional privileges<br />
simply let them edit other project records.<br />
Any other direct access requires logging in to the production system<br />
through a separate log in (e.g., to use 'rails console').<br />
Indirect access (e.g., to update the code the site runs)<br />
requires separately logging into<br />
GitHub and performing a valid git push (this must also pass through the<br />
continuous integration test suite).<br />
It's possible to directly push to the Heroku sites to deploy software,<br />
but this requires the credentials for directly logging into the<br />
relevant tier (e.g., production), and only authorized system administrators<br />
have those credentials.</li>
<li><em>User management.</em><br />
Local passwords have a minimum length (8) and cannot be<br />
a member of a set of known-bad passwords.  We allow much longer passwords.<br />
This complies with NIST Special Publication 800-63B,<br />
&quot;Digital Authentication Guideline: Authentication and Lifecycle Management&quot;<br />
<a href="https://pages.nist.gov/800-63-3/sp800-63b.html">https://pages.nist.gov/800-63-3/sp800-63b.html</a> section 5.1.1.2.<br />
We expect users to<br />
protect their own passwords; we do not try to protect users from themselves.<br />
The system is not fast enough for a naive password-guesser to succeed<br />
guessing local passwords via network access (unless the password<br />
is really bad).<br />
The forgotten-password system for local accounts<br />
uses email; that has its weaknesses,<br />
but the data is sufficiently low value, and there aren't<br />
good alternatives for low value data like this.<br />
This isn't as bad as it might appear, because we prefer encrypted<br />
channels for transmitting all emails.<br />
Historically our application attempts to send<br />
messages to its MTA using TLS (using <code>enable_starttls_auto: true</code>),<br />
and that MTA then attempts to transfer the email the rest<br />
of the way using TLS if the recipient's email system supports it<br />
(see <a href="https://sendgrid.com/docs/Glossary/tls.html">https://sendgrid.com/docs/Glossary/tls.html</a>).<br />
This is good protection against passive attacks, and is relatively decent<br />
protection against active attacks if the user chooses an email system<br />
that supports TLS (an active attacker has to get between the email<br />
MTAs, which is often not easy).<br />
More recently we're using <code>enable_starttls: true</code> which <em>forces</em> email to be<br />
encrypted point-to-point.<br />
If users don't like that, they can log in via GitHub and use GitHub's<br />
system for dealing with forgotten passwords.<br />
The file <code>config/initializers/filter_parameter_logging.rb</code><br />
intentionally filters passwords so that they are not included in the log.<br />
We require that local user passwords have a minimum length<br />
(see the User model), and this is validated by the server<br />
(in some cases the minimum length is also checked by the web client,<br />
but this is not depended on).<br />
Ruby's regular expression (regex) language oddly interprets &quot;^&quot; and &quot;$&quot;,<br />
which can lead to defects (you're supposed to use \A and \Z instead).<br />
However, Ruby's format validator and the &quot;Brakeman&quot; tool both detect<br />
this common mistake with regexes, so this should be unlikely.<br />
Since the project data is public, manipulating the 'id' cannot reveal<br />
private public data.  We don't consider the list of valid users<br />
private either, so again, manipulating 'id' cannot reveal anything private.</li>
<li><em>Injection.</em><br />
We use allowlists to validate project data entered into the system.<br />
When acquiring data from projects during autofill, we do only for the<br />
presence or absence of patterns; the data is not stored (other than caching)<br />
and the data is not used in command interpreters (such as SQL or shell).<br />
SQL injection is countered by Rails' built-in database query mechanisms,<br />
we primarily use specialized routines like find() that counter<br />
SQL injection, but parameterized queries are also allowed for untrusted data<br />
(and also counter SQL injection).<br />
XSS, CSS injection, and Ajax injection are<br />
countered using Rails' HTML sanitization<br />
(by default strings are escaped when generating HTML)<br />
and our markdown generator.<br />
The program doesn't call out to the command line or use a routine<br />
that directly does so, e.g., there's no call<br />
to system()... so command injection won't work either.<br />
The software resists header injection including response splitting;<br />
headers are typically not dynamically generated, most redirections<br />
(using <code>redirect_to</code>) are to static locations, and the rest are based<br />
on filtered locations.<br />
We use a restrictive CSP setting to limit damage if all those fail.</li>
<li><em>Unsafe Query Generation.</em><br />
We use the default Rails behavior, in particular, we leave<br />
<code>deep_munge</code> at its default value<br />
(this default value counters a number of vulnerabilities).</li>
<li><em>Default Headers.</em><br />
We use at least the default security HTTP headers,<br />
which help counter some attacks.<br />
We harden the headers further, in particular via the<br />
<a href="https://github.com/twitter/secureheaders"><code>secure_headers</code></a> gem.<br />
For example, we use a restrictive Content Security Policy (CSP) header.<br />
For more information, see the hardening section.</li>
</ol>
<p>The<br />
<a href="http://guides.rubyonrails.org/security.html">Ruby on Rails Security Guide</a><br />
is the official Rails guide, so it is the primary guide we consult.<br />
That said, we do look for other sources for recommendations,<br />
and consider them where they make sense.</p>
<p>In particular, the<br />
<a href="https://github.com/ankane/secure_rails"><code>ankane/secure_rails</code></a> guide<br />
has some interesting tips.<br />
Most of them were were already doing, but an especially interesting<br />
tip was to<br />
&quot;Prevent host header injection -<br />
add the following to config/environments/production.rb&quot;:</p>
<pre lang="ruby" style="background-color:#2b303b;"><code><span style="color:#c0c5ce;">config.action_controller.default_url_options = {</span><span style="color:#a3be8c;">host: </span><span style="color:#c0c5ce;">&quot;</span><span style="color:#a3be8c;">www.yoursite.com</span><span style="color:#c0c5ce;">&quot;}
</span><span style="color:#c0c5ce;">config.action_controller.asset_host = &quot;</span><span style="color:#a3be8c;">www.yoursite.com</span><span style="color:#c0c5ce;">&quot;
</span></code></pre>
<p>We already did the first one, but we also added the second.</p>
<p>In many Rails configurations this is a critically-required configuration,<br />
and failing to follow these steps could lead in part<br />
to a potential compromise.<br />
In our particular configuration the host value is set<br />
by a trusted entity (Heroku), so we were never vulnerable,<br />
but there is no reason to depend on the value Heroku provides.<br />
We know the correct values, so we forcibly set them.<br />
This ensures that even if a user provides a &quot;host&quot; value,<br />
and for some reason Heroku allows it to pass through or we<br />
switch to a different computation engine provider,<br />
we will not use this value; we will instead use a preset trusted value.</p>
<h3><a href="#hardening" aria-hidden="true" class="anchor" id="hardening"></a>Hardening</h3>
<p>We also use various mechanisms to harden the system against attack.<br />
These attempt to thwart or slow attack even if the system has a vulnerability<br />
not countered by the main approaches described elsewhere in this document.</p>
<h4><a href="#force-the-use-of-https-including-via-hsts" aria-hidden="true" class="anchor" id="force-the-use-of-https-including-via-hsts"></a>Force the use of HTTPS, including via HSTS</h4>
<p>We take a number of steps to force the use of HTTPS instead of HTTP.</p>
<p>The &quot;coreinfrastructure.org&quot; domain is included in<br />
<a href="https://hstspreload.org/?domain=coreinfrastructure.org">Chrome's HTTP Strict Transport Security (HSTS) preload list</a>.<br />
This is a list of sites that are hardcoded into Chrome as being HTTPS only<br />
(some other browsers also use this list), so in many cases browsers<br />
will automatically use HTTPS (even if HTTP is requested).</p>
<p>If the web browser uses HTTP anyway,<br />
our CDN (Fastly) is configured to redirect HTTP to HTTPS.<br />
If our CDN is misconfigured or skipped for some reason, the application<br />
will also redirect the user from HTTP to HTTPS if queried directly.<br />
This is because in production <code>config.force_ssl</code> is set to true,<br />
which enables a number of hardening mechanisms in Rails, including<br />
TLS redirection (which redirects HTTP to HTTPS), secure cookies,<br />
and HTTP Strict Transport Security (HSTS).<br />
HSTS tells browsers to always use HTTPS in the future for this site,<br />
so once the user contacts the site once, it will use HTTPS in the future.<br />
See<br />
<a href="http://eftimov.net/rails-tls-hsts-cookies">&quot;Rails, Secure Cookies, HSTS and friends&quot; by Ilija Eftimov (2015-12-14)</a><br />
for more about the impact of <code>force_ssl</code>.</p>
<h4><a href="#hardened-outgoing-http-headers-including-restrictive-csp" aria-hidden="true" class="anchor" id="hardened-outgoing-http-headers-including-restrictive-csp"></a>Hardened outgoing HTTP headers, including restrictive CSP</h4>
<p>We harden the outgoing HTTP headers, in particular, we use a<br />
restrictive Content Security Policy (CSP) header with just<br />
&quot;normal sources&quot; (<code>normal_src</code>).  We do send a<br />
Cross-Origin Resource Sharing (CORS) header when an origin is specified,<br />
but the CORS header does <em>not</em> share credentials.</p>
<p>CSP is perhaps one of the most important hardening items,<br />
since it prevents execution of injected JavaScript).<br />
The HTTP headers are hardened via the<br />
<a href="https://github.com/twitter/secureheaders"><code>secure_headers</code></a> gem,<br />
developed by Twitter to enable a number of HTTP headers for hardening.<br />
We check that the HTTP headers are hardened in the test file<br />
<code>test/integration/project_get_test.rb</code>; that way, when we upgrade<br />
the <code>secure_headers</code> gem, we can be confident that the headers continue to<br />
be restrictive.<br />
The test checks for the HTTP header values when loading a project entry,<br />
since that is the one most at risk from user-provided data.<br />
That said, the hardening HTTP headers are basically the same for all<br />
pages except for <code>/project_stats</code>, and that page doesn't display<br />
any user-provided data.<br />
We have separately checked the CSP values we use with<br />
<a href="https://csp-evaluator.withgoogle.com/">https://csp-evaluator.withgoogle.com/</a>;<br />
the only warning it mentioned is that the our &quot;default-src&quot; allows 'self',<br />
and it notes that<br />
&quot;'self' can be problematic if you host JSONP, Angular<br />
or user uploaded files.&quot;  That is true, but irrelevant, because we don't<br />
host any of them.</p>
<p>The HTTP headers <em>do</em> include a<br />
Cross-Origin Resource Sharing (CORS) header when an origin is specified.<br />
We do this so that client-side JavaScript served by other systems can<br />
acquire data directly from our site (e.g., to download JSON data to<br />
extract and display).<br />
CORS disables the usual shared-origin policy, which is always a concern.<br />
However, the CORS header expressly does <em>not</em> share credentials, and<br />
our automated tests verify this (both when an origin is sent, and when<br />
one is not).  The CORS header <em>only</em> allows GET; while an attacker <em>could</em><br />
set the method= attribute, that wouldn't have any useful effect, because<br />
the attacker won't have credentials (except for themselves, and<br />
attackers can always change the data they legitimately have rights to<br />
on the BadgeApp).<br />
A CORS header does make it <em>slightly</em> easier to perform<br />
a DDoS attack (since JavaScript clients can make excessive data demands),<br />
but a DDoS attack can be performed without it, and our usual DDoS<br />
protection measures (including caching and scaling) still apply.</p>
<h4><a href="#cookie-limits" aria-hidden="true" class="anchor" id="cookie-limits"></a>Cookie limits</h4>
<p>Cookies have various restrictions (also via the<br />
<a href="https://github.com/twitter/secureheaders"><code>secure_headers</code></a> gem).<br />
They have httponly=true (which counters many JavaScript-based attacks),<br />
secure=true (which is irrelevant because we always use HTTPS but it<br />
can't hurt), and SameSite=Lax (which counters CSRF attacks on<br />
web browsers that support it).</p>
<p>We also use the Rails 5.2 default setting which embeds<br />
the expiry information in encrypted or signed cookies value to<br />
improve security.<br />
Embedding and checking expiration data<br />
makes it harder to exploit these cookies.<br />
See<br />
<a href="https://edgeguides.rubyonrails.org/upgrading_ruby_on_rails.html#expiry-in-signed-or-encrypted-cookie-is-now-embedded-in-the-cookies-values">expiry in signed or encrypted cookie is now embedded in the cookies values</a>.</p>
<h4><a href="#csrf-token-hardening" aria-hidden="true" class="anchor" id="csrf-token-hardening"></a>CSRF token hardening</h4>
<p>We use two additional CSRF token hardening techniques<br />
to further harden the system against CSRF attacks.<br />
Both of these techniques are Rails 5 additions:</p>
<ul>
<li>We enable per-form CSRF tokens<br />
(<code>Rails.application.config.action_controller.</code> <code>per_form_csrf_tokens</code>).</li>
<li>We enable origin-checking CSRF mitigation<br />
(<code>Rails.application.config.action_controller.</code><br />
<code>forgery_protection_origin_check</code>).</li>
</ul>
<p>These help counter CSRF, in addition to our other measures.</p>
<h4><a href="#incoming-rate-limits" aria-hidden="true" class="anchor" id="incoming-rate-limits"></a>Incoming rate limits</h4>
<p>Rate limits provide an automated partial<br />
countermeasure against denial-of-service and password-guessing attacks.<br />
These are implemented by Rack::Attack and have two parts, a<br />
&quot;LIMIT&quot; (maximum count) and a &quot;PERIOD&quot; (length of period of time,<br />
in seconds, where that limit is not to be exceeded).<br />
If unspecified they have the default values specified in<br />
<code>config/initializers/rack_attack.rb</code>.  These settings are<br />
(where &quot;IP&quot; or &quot;ip&quot; means &quot;client IP address&quot;, and &quot;req&quot; means &quot;requests&quot;):</p>
<ul>
<li>req/ip</li>
<li>logins/ip</li>
<li>logins/email</li>
<li>signup/ip</li>
</ul>
<p>We also have a set of simple FAIL2BAN settings that temporarily<br />
bans an IP address if it makes too many &quot;suspicious&quot; requests.<br />
The exact production settings are not documented here, since we<br />
don't want to tell attackers what we look for.<br />
This isn't the same thing as having a <em>real</em><br />
web application firewall, but it's simple and counters some<br />
trivial attacks.</p>
<p>To determine the remote client IP address (for our purposes) we use the<br />
the next-to-last value of the comma-space-separated value<br />
<code>HTTP_X_FORWARDED_FOR</code> (from the HTTP header X-Forwarded-For).<br />
That's because the last value of <code>HTTP_X_FORWARDED_FOR</code><br />
is always our CDN (which intercepts it first), and the previous<br />
value is set by our CDN to whatever IP address the CDN got.<br />
The web server is configured so it will only accept connections from the<br />
CDN - this prevents web piercing, and means that we can trust that the<br />
client IP value we receive is only from the CDN (which we trust for<br />
this purpose).<br />
A client can always set X-Forwarded-For and try to spoof something,<br />
but those entries are always earlier in the list<br />
(so we can easily ignore them).</p>
<h4><a href="#outgoing-email-rate-limit" aria-hidden="true" class="anchor" id="outgoing-email-rate-limit"></a>Outgoing email rate limit</h4>
<p>We enable rate limits on outgoing reminder emails.<br />
We send reminder emails to projects that have not updated their<br />
badge entry in a long time. The detailed algorithm that prioritizes projects<br />
is in <code>app/models/project.rb</code> class method <code>self.projects_to_remind</code>.<br />
It sorts by reminder date, so we always cycle through before returning to<br />
a previously-reminded project.</p>
<p>We have a hard rate limit on the number of emails we will send out each<br />
time; this keeps us from looking like a spammer.</p>
<h4><a href="#encrypted-email-addresses" aria-hidden="true" class="anchor" id="encrypted-email-addresses"></a>Encrypted email addresses</h4>
<p>We encrypt email addresses within the database, and never<br />
send the decryption or index keys to the database system.<br />
This provides protection of this data at rest, and also means that<br />
even if an attacker can view the data within the database, that attacker<br />
will not receive sensitive information.<br />
Email addresses are encrypted as described here, and almost all other<br />
data is considered public or at least not sensitive<br />
(the exception are passwords, which are<br />
specially encrypted as described above).</p>
<p>A little context may be useful here.<br />
We work hard to comply with various privacy-related regulations,<br />
including the European General Data Protection Regulation (GDPR).<br />
We do not believe that encrypting email addresses is strictly<br />
required by the GDPR.<br />
Still, we want to not just meet requirements, we want to exceed them.<br />
Encrypting email addresses makes it even harder for attackers to get this<br />
information, because it's encrypted at rest and not available by extracting<br />
data from the database system.</p>
<p>First, it is useful to note why we encrypt just email addresses<br />
(and passwords), and not all data.<br />
Most obviously, almost all data we manage is public anyway.<br />
In addition,<br />
the easy ways to encrypt data aren't available to us. Transparent Data<br />
Encryption (TDE) is not a capability of PostgreSQL. Whole-database<br />
encryption can be done with other tricks but it is extremely expensive<br />
on Heroku.<br />
Therefore, we encrypt data that is more sensitive, instead of<br />
encrypting everything.</p>
<p>We encrypt email addresses using the Rails-specific approach outlined in<br />
<a href="https://shorts.dokkuapp.com/securing-user-emails-in-rails/">&quot;Securing User Emails in Rails&quot; by Andrew Kane (May 14, 2018)</a>.<br />
We use the gem <code>attr_encrypted</code> to encrypt email addresses, and<br />
gem <code>blind_index</code> to index encrypted email addresses.<br />
This approach builds on standard general-purpose approaches for<br />
encrypting data and indexing the data, e.g., see<br />
<a href="https://www.sitepoint.com/how-to-search-on-securely-encrypted-database-fields/">&quot;How to Search on Securely Encrypted Database Fields&quot; by Scott Arciszewski</a>.<br />
The important aspect here is that we encrypt the data (so it cannot be<br />
revealed by those without the encryption key),<br />
and we also create cryptographic keyed hashes of the data (so<br />
we can search on the data if we have the hash key).<br />
The latter value is called a &quot;blind index&quot;.</p>
<p>We encrypt the email addresses using AES with 256-bit keys in<br />
GCM mode ('aes-256-gcm').  AES is a well-accepted widely-used<br />
encryption algorithm.  A 256-bit key is especially strong.<br />
The GCM mode is a widely-used strong encryption mode; it provides<br />
an integrity (&quot;authentication&quot;) mechanism.<br />
Each separate encryption uses a separate long initialization vector (IV)<br />
created using a cryptographically-strong random number generator.</p>
<p>We also hash the email addresses, so they can be indexed.<br />
Indexing is necessary so that we can quickly find matching email addresses<br />
(e.g., for local user login).<br />
We hash them using the hashed key algorithm PBKDF2-HMAC-SHA256.<br />
SHA-256 is a widely-used cryptographic hash algorithm (in the SHA-2 family),<br />
and unlike SHA-1 it is not broken.<br />
Using sha256 directly would be vulnerable to a length extension attack.<br />
A length extension attack is probably irrelevant in this circumstance,<br />
but just in case, we counter that anyway.<br />
We counter the length extension problem by using HMAC and PBKDF2.<br />
HMAC is defined in RFC 2104, which is the algorithm<br />
H(K XOR opad, H(K XOR ipad, text)).<br />
This enables us to use a private key on the hash, counters length<br />
extension, and is very well-studied.<br />
We also use PBKDF2 for key extension.  This is another well-studied<br />
and widely-accepted algorithm.<br />
For our purposes we believe PBKDF2-HMAC-SHA256 is<br />
far stronger than needed, and thus is quite sufficient<br />
to protect the information.<br />
The hashes are of email addresses after they've been downcased;<br />
this supports case-insensitive searching for email addresses.</p>
<p>The two keys used for email encryption are<br />
<code>EMAIL_ENCRYPTION_KEY</code> and <code>EMAIL_BLIND_INDEX_KEY</code>.<br />
Both are 256 bits long (aka 64 hexadecimal digits long).<br />
The production values for both keys were independently created as<br />
cryptographically random values using <code>rails secret</code>.</p>
<p>Implementation note: the indexes created by <code>blind_index</code> always<br />
end in a newline.  That doesn't matter for security, but it can cause<br />
debugging problems if you weren't expecting that.</p>
<p>Note that <code>attr_encrypted</code> depends on the gem <code>encryptor</code>.<br />
Encryptor version 2.0.0 had a<br />
<a href="https://github.com/attr-encrypted/encryptor/pull/22">major security bug when using AES-<span data-escaped-char>*</span>-GCM algorithms</a>.<br />
We do not use that version, but instead use<br />
a newer version that does not have that vulnerability.<br />
Some old documentation recommends using<br />
<code>attr_encryptor</code> instead because of this vulnerability, but the<br />
vulnerability has since been fixed and<br />
<code>attr_encryptor</code> is no longer maintained.<br />
Vulnerabilities are never a great sign, but we do take it as a good sign<br />
that the developers of encryptor were willing to make a breaking change<br />
to fix a security vulnerabilities.</p>
<p>We could easily claim this as a way to support confidentiality,<br />
instead of simply as a hardening measure.<br />
We only claim email encryption<br />
as a hardening measure because we must still support<br />
two-way encryption and decryption, and the keys must remain available<br />
to the application.<br />
As a result, email encryption only counters some specific attack methods.<br />
That said, we believe this encryption adds an additional layer of defense<br />
to protect email addresses from being revealed.</p>
<h4><a href="#gravatar-restricted" aria-hidden="true" class="anchor" id="gravatar-restricted"></a>Gravatar restricted</h4>
<p>We use gravatar to provide user icons for local (custom) accounts.<br />
Many users have created gravatar icons, and those who have<br />
created those icons have clearly consented to their use for them.</p>
<p>However, accessing gravatar icons requires the MD5 cryptographic<br />
hash of downcased email addresses.<br />
Users who have created gravatar icons have already consented to<br />
this, but we want to hide even the MD5 cryptographic hashes of<br />
those who have not so consented.</p>
<p>Therefore, we track for each user whether or not they should<br />
use a gravatar icon, as the boolean field <code>use_gravatar</code>.<br />
Currently this is can only be true for<br />
local users (for GitHub users we use their GitHub icon).<br />
Whenever a new local user account is created or changed,<br />
we check if there is an active gravatar icon, and set use_gravatar<br />
accordingly.  We also intend to occasionally iterate through<br />
local users to reset this (so that users won't need to remember<br />
to manipulate their BadgeApp user account).<br />
We will then only use the gravatar MD5 when there is an<br />
actual gravatar icon to refer to; otherwise, we use a bogus<br />
MD5 value.<br />
Thus, local users who do not have a gravatar account will not<br />
even have the MD5 of their email address revealed.</p>
<p>This is almost certainly not required by regulations such as the GDPR,<br />
since without this measure we would only expose MD5s of email addresses,<br />
and only in certain cases.  But we want to exceed expectations,<br />
and this is one way we do that.</p>
<h3><a href="#making-adjustments" aria-hidden="true" class="anchor" id="making-adjustments"></a>Making adjustments</h3>
<p>We want to counter all common vulnerabilities, not just those<br />
listed in the OWASP top 10 or those mentioned in the configuration guide.<br />
Therefore, we monitor information to learn about new types of vulnerabilities,<br />
and make adjustments as necessary.</p>
<p>For example, a common vulnerability not reported in the 2013 OWASP top 10<br />
is the use of <code>target=</code> in the &quot;a&quot; tag that does not have <code>_self</code><br />
as its value.<br />
This is discussed in, for example,<br />
<a href="https://www.jitbit.com/alexblog/256-targetblank---the-most-underestimated-vulnerability-ever/">&quot;Target=&quot;<span data-escaped-char>_</span>blank&quot; - the most underestimated vulnerability ever&quot; by Alex Yumashev, May 4, 2016</a>.<br />
This was not noted in the OWASP top 10 of 2013,<br />
which is unsurprising, since the problem with target=<br />
was not widely known until 2016.<br />
Note that no one had to report the vulnerability about this particular<br />
application; we noticed it on our own.</p>
<p>Today we discourage the use of target=, because removing target= completely<br />
eliminates the vulnerability.  When target= is used,<br />
which is sometimes valuable to avoid the risk of user data loss,<br />
we require that <code>rel=&quot;noopener&quot;</code> always be used with <code>target=</code><br />
(this is the standard mitigation for <code>target=</code>).</p>
<p>We learned about this type of vulnerability after the application was<br />
originally developed, through our monitoring of sites that discuss<br />
general vulnerabilities.<br />
To address the <code>target=</code> vulnerability, we:</p>
<ul>
<li>modified the application to counter the vulnerability,</li>
<li>documented in CONTRIBUTING.md that it's not acceptable to have bare target=<br />
values (we discourage their use, and when they need to be used, they<br />
must be used with rel=&quot;noopener&quot;)</li>
<li>modified the translation:sync routine to automatically insert the<br />
<code>rel=&quot;noopener&quot;</code> mitigations for all target= values when they aren't<br />
already present</li>
<li>modified the test suite to try to detect unmitigated uses of target=<br />
in key pages (the home page, project index, and single project page)</li>
<li>modified the test suit to examine all text managed by config/locales<br />
(this is nearly all text) to detect use of target= with an immediate<br />
termination (this is the common failure mode, since rel=... should<br />
instead follow it).</li>
</ul>
<p>While this doesn't <em>guarantee</em> there is no vulnerability, this certainly<br />
reduces the risks.</p>
<h3><a href="#securely-reuse-supply-chain" aria-hidden="true" class="anchor" id="securely-reuse-supply-chain"></a><a name="reuse"></a><a name="supply-chain"></a>Securely reuse (supply chain)</h3>
<p>Like all modern software, we reuse components developed by others.<br />
We can't eliminate all risks, and<br />
if we rewrote all the software (instead of reusing software)<br />
we would risk creating vulnerabilities in own code.<br />
See <a href="../CONTRIBUTING.html">CONTRIBUTING.md</a> for more about how we<br />
reduce the risks of reused code.</p>
<h4><a href="#review-before-use" aria-hidden="true" class="anchor" id="review-before-use"></a>Review before use</h4>
<p>We consider the code we reuse<br />
(e.g., libraries and frameworks) before adding them, to reduce<br />
the risk of unintentional and intentional vulnerabilities from them.<br />
In particular:</p>
<ul>
<li>We look at the website of any component we intend to add to our<br />
direct dependencies to see if it appears to be relatively low risk<br />
(does it have clear documentation, is there evidence of serious<br />
security concerns, does it have multiple developers, who is the lead, etc.).</li>
<li>We prefer the use of popular components (where problems<br />
are more likely to be identified and addressed).</li>
<li>We prefer software that self-proclaims a version number of 1.0 or higher.</li>
<li>We strongly prefer software that does not bring in a large number<br />
of new dependencies (as determined by bundler).<br />
If it is a large number, we consider even more carefully<br />
and/or suggest that the developer reduce their dependencies<br />
(often the dependencies are only for development and test).</li>
<li>In some cases we review the code ourselves.  This primarily happens<br />
when there are concerns raised by one of the previous steps.</li>
</ul>
<p>We require that all components that are <em>required</em> for use<br />
have FLOSS licenses.  This enables review by us and by others.<br />
We prefer common FLOSS licenses.<br />
A FLOSS component with a rarely-used license, particularly a<br />
GPL-incompatible one, is less likely to be reviewed by others because<br />
in most cases fewer people will contribute to it.<br />
We use <code>license_finder</code> to ensure that the licenses are what we expect,<br />
and that the licenses do not change to an unusual license<br />
in later versions.</p>
<h3><a href="#get-authentic-version" aria-hidden="true" class="anchor" id="get-authentic-version"></a>Get authentic version</h3>
<p>We work to ensure that we are getting the authentic version of the software.<br />
We counter man-in-the-middle (MITM) attacks when downloading gems<br />
because the Gemfile configuration uses an HTTPS source to the<br />
standard place for loading gems (<a href="https://rubygems.org">https://rubygems.org</a>).<br />
We double-check names before we add them to the Gemfile to counter<br />
typosquatting attacks.</p>
<h3><a href="#use-package-manager" aria-hidden="true" class="anchor" id="use-package-manager"></a>Use package manager</h3>
<p>We use package managers, primarily bundler, to download and track<br />
software with the correct version numbers.<br />
This makes it much easier to maintain the software later<br />
(see the maintenance process discussion).</p>
<h3><a href="#special-analysis" aria-hidden="true" class="anchor" id="special-analysis"></a>Special analysis</h3>
<p>Sometimes tools or reports suggest we may have vulnerabilities<br />
in how we reuse components.<br />
We are grateful for those tools and reports that help us identify<br />
places to us to examine further!<br />
Below are specialized analysis justifying why we believe we do not<br />
have vulnerabilities in these areas.</p>
<h4><a href="#xxe-from-nokogiri--libxml2" aria-hidden="true" class="anchor" id="xxe-from-nokogiri--libxml2"></a>XXE from Nokogiri / libxml2</h4>
<p>One area where we may <em>appear</em> to be vulnerable, but we<br />
believe we are not, involves nokogiri, libxml2, and<br />
<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-9318">CVE-2016-9318</a>.<br />
This was identified as a potential issue in an<br />
<a href="https://snyk.io/test/github/coreinfrastructure/best-practices-badge?severity=high&amp;severity=medium&amp;severity=low">analysis by Snyk</a>.<br />
Here is our analysis justifying why we believe our use of<br />
nokogiri and libxml2 are not a vulnerability in our application.</p>
<p>The Nokogiri gem is able to do analysis on HTML and XML documents.<br />
As noted in<br />
<a href="https://github.com/sparklemotion/nokogiri/issues/1582">Nokogiri #1582</a>,<br />
&quot;applications using Nokogiri 1.5.4 (circa June 2012) and later<br />
are not vulnerable to this CVE (CVE-2016-9318)<br />
unless opting into the DTDLOAD option and opting out of the NONET option.<br />
Note that by default, Nokogiri turns off both DTD loading and<br />
network access, treating all docs as untrusted docs. This behavior<br />
is independent of the version of libxml2 used.&quot;</p>
<p>Like practically all Rails applications, we use<br />
rails-html-sanitizer, which uses loofah, which uses nokogiri,<br />
which uses libxml.<br />
So we <em>do</em> use Nokogiri in production to process untrusted data.<br />
However, we do <em>not</em> use Nokogiri in production to process incoming XML,<br />
which is the only way this vulnerability can occur.<br />
In addition, loofah version 2.1.1 has been checked and it never opts<br />
into DTDLOAD nor does it opt out of NONET<br />
(as confirmed using &quot;grep -Ri NONET&quot; and &quot;grep -Ri DTDLOAD&quot; on<br />
the loofah source directory), so nokogiri is never configured in<br />
production in a way that could be vulnerable anyway.</p>
<p>All other uses of nokogiri (as confirmed by checking Gemfile.lock)<br />
are only for the automated test suite<br />
(capybara, chromedriver-helper, rails-dom-testing, and xpath which in<br />
turn is only used by capybara).<br />
Nokogiri is extensively used by our automated testing system,<br />
but in that case we provide the test data to ourselves (so it is trusted).</p>
<h4><a href="#xss-from-erubis--pronto-rails_best_practices" aria-hidden="true" class="anchor" id="xss-from-erubis--pronto-rails_best_practices"></a>XSS from erubis / pronto-rails_best_practices</h4>
<p>The &quot;erubis&quot; module was identified as potentially vulnerable to<br />
cross-site scripting (XSS) as introduced through<br />
<code>pronto-rails_best_practices</code>.<br />
This was identified as a potential issue in an<br />
<a href="https://snyk.io/test/github/coreinfrastructure/best-practices-badge?severity=high&amp;severity=medium&amp;severity=low">analysis by Snyk</a>.</p>
<p>However, rails_best_practices is only run in development and test,<br />
where the environment and input data are trusted,<br />
so we believe this is not a real vulnerability.</p>
<h4><a href="#checked-in-tmplocal_secrettxt-appears-to-have-a-secret" aria-hidden="true" class="anchor" id="checked-in-tmplocal_secrettxt-appears-to-have-a-secret"></a>Checked-in <code>tmp/local_secret.txt</code> appears to have a secret</h4>
<p>The file <code>tmp/local_secret.txt</code> may appear to be a security vulnerability.<br />
That's because it's a key (it's the value of <code>secret_key_base</code> for<br />
development and test environments) whose value is checked into a public<br />
version-controlled repository.</p>
<p>However, it is not a vulnerability. This value is <em>only</em> used during<br />
test and development. The fact that its value is public is<br />
irrelevant, since those systems are not publicly accessible, or don't<br />
have any real secrets, or both.</p>
<p>We set this file for the following reasons;</p>
<ol>
<li>It's convenient to have a <em>constant</em> value of<br />
<code>secret_key_base</code>, because it means it's much easier to halt and<br />
restart during development and test. Having it change on every restart<br />
makes development and test much harder.</li>
<li>It eliminates occasional test failures in parallelized testing.<br />
At the time of this writing, when Rails needs a value for <code>secret_key_base</code><br />
and cannot find it in the environment variable <code>SECRET_KEY_BASE</code> or a file,<br />
it creates a new value for <code>secret_key_base</code> and stores it in the file<br />
<code>tmp/local_secret.txt</code>.<br />
<a href="https://github.com/rails/rails/issues/53661">A race condition results</a>,<br />
because if tests are running in parallel, other tests may try to load the<br />
file before it's been created by another process.<br />
I've verified in the Rails code that if this file exists,<br />
its content is used.</li>
</ol>
<p>In the staging and production environments we provide a truly secret value for<br />
the environment variable <code>SECRET_KEY_BASE</code>. When this environment variable<br />
is set, it is used instead. Their values are the ones<br />
we actually need to keep private. We do <em>not</em> check in their values.</p>
<p>For more information:</p>
<ul>
<li><a href="https://apidock.com/rails/Rails/Application/secret_key_base">https://apidock.com/rails/Rails/Application/secret_key_base</a></li>
<li><a href="https://apidock.com/rails/v7.1.3.2/Rails/Application/generate_local_secret">https://apidock.com/rails/v7.1.3.2/Rails/Application/generate_local_secret</a></li>
</ul>
<h4><a href="#information-exposure-from-actioncable" aria-hidden="true" class="anchor" id="information-exposure-from-actioncable"></a>Information Exposure from actioncable</h4>
<p>The &quot;actioncable&quot; module was identified as potentially vulnerable to<br />
information exposure.<br />
This module is introduced by rails and traceroute.<br />
This was identified as a potential issue in an<br />
<a href="https://snyk.io/test/github/coreinfrastructure/best-practices-badge?severity=high&amp;severity=medium&amp;severity=low">analysis by Snyk</a>.</p>
<p>Actioncable structures channels over a single WebSocket connection,<br />
however, there is<br />
<a href="https://github.com/rails/rails/issues/25088">no way to filter out any sensitive data from the logs</a>.</p>
<p>We don't currently use actioncable, and since we don't use it, there's<br />
no sensitive data going over it to worry about.</p>
<p>We could remove actioncable as a dependency, but that turns out to be<br />
annoying.<br />
The solution would be to replace the &quot;rails&quot; dependency with a large set<br />
of its transitive dependencies and then removing actioncable.<br />
Traceroute could be requested in the development group, and then<br />
the development group could re-require rails.<br />
This would make later maintenance a little more difficult, with no<br />
obvious gain, so we have not done this.</p>
<h2><a href="#security-in-integration-and-verification" aria-hidden="true" class="anchor" id="security-in-integration-and-verification"></a>Security in integration and verification</h2>
<p>When software is modified, it is reviewed by the<br />
'rake' process, which performs a number of checks and tests.<br />
Modifications integrated into the master branch<br />
are further automatically checked.<br />
See <a href="../CONTRIBUTING.html">CONTRIBUTING.md</a> for more information.</p>
<p>The following is a brief summary of part of our verification process,<br />
and how it helps make the software more secure:</p>
<ul>
<li>Style checking tools.<br />
We intentionally make the code relatively short and clean to ease review<br />
by both humans and other tools.<br />
We use rubocop (a Ruby code style checker), rails_best_practices<br />
(a style checker specific to Rails), and ESLint<br />
(a style checker for JavaScript).<br />
We work to have no warnings in the code,<br />
typically by fixing the problem, though in some cases we will annotate<br />
in the code that we're allowing an exception.<br />
These style tools help us avoid more problematic constructs (in some cases<br />
avoiding defects that might lead to vulnerabilities), and<br />
also make the code easier to review<br />
(by both humans and other programs).<br />
Our style checking tools detect misleading indentation;<br />
<a href="http://www.dwheeler.com/essays/apple-goto-fail.html#indentation">this<br />
counters the mistake in the Apple goto fail vulnerability</a>.</li>
<li>Source code weakness analyzers (for finding vulnerabilities in custom code).<br />
A source code weakness analyzer, also known as a security vulnerability<br />
scanner, examines the source code to identify vulnerabilities.<br />
This is one of many kinds of &quot;static analysis&quot; tools, that is, a tool<br />
that doesn't run the code (and thus is not limited to examining only the<br />
cases of specific inputs).<br />
We use Brakeman, a source code weakness analyzer that focuses<br />
on finding security issues in Ruby on Rails applications.<br />
We also use CodeQL, another source code weakness analyzer that's<br />
general-purpose and reviews both the Ruby and JavaScript code.<br />
Note that this is all separate from the automatic detection of<br />
third-party components with publicly-known vulnerabilities;<br />
see the <a href="#supply-chain">supply chain</a> section for how we counter those.</li>
<li>FLOSS.  Reviewability is important for security.<br />
All the required reused components are FLOSS, and our<br />
custom software is released as Free/Libre and open source software (FLOSS)<br />
using a well-known FLOSS license (MIT).</li>
<li>Negative testing.<br />
The test suite specifically includes tests that should fail for<br />
security reasons, an approach sometimes called &quot;negative testing&quot;.<br />
A widespread mistake in test suites is to only test &quot;things that should<br />
succeed&quot;, and neglecting to test &quot;things that should fail&quot;.<br />
This is especially important in security, since for security it's<br />
often more important to ensure that certain requests <em>fail</em> than to ensure<br />
that certain requests <em>succeed</em>.<br />
For an example of the need for negative testing, see<br />
<a href="https://www.dwheeler.com/essays/apple-goto-fail.html">&quot;The Apple goto fail vulnerability: lessons learned&quot; by David A. Wheeler</a>.<br />
Missing negative tests are also problematic because<br />
statement and branch coverage test coverage requirements<br />
cannot detect <em>missing</em> code, and &quot;failure to fail&quot; is often caused<br />
by <em>missing</em> code (this wasn't the case for &quot;goto fail&quot;, but it does happen<br />
in other cases).<br />
We do positive testing too, of course, but that's not usually forgotten.<br />
For negative testing, we focus on ensuring that incorrect logins will<br />
fail, that timeouts cause timeouts, that projects and users cannot be<br />
edited by those unauthorized to do so, and that email addresses are not<br />
revealed to unauthorized individuals.<br />
Here are important examples of our negative testing:
<ul>
<li>local logins with wrong or unfilled passwords will lead to login failure<br />
(see <code>test/system/login_test.rb</code>).</li>
<li>projects cannot be edited (&quot;patched&quot;) by a timed-out session<br />
or a session lacking a signed timeout value<br />
(see <code>test/controllers/projects_controller_test.rb</code>)</li>
<li>projects cannot be edited if the user is not logged in, or<br />
by logged-in normal users<br />
if they aren't authorized to edit that project<br />
(see <code>test/controllers/projects_controller_test.rb</code>)</li>
<li>projects can't be destroyed (deleted) if the user isn't logged in,<br />
or is logged as a user who does not control the project<br />
(see <code>test/controllers/projects_controller_test.rb</code>)</li>
<li>user data cannot be edited (&quot;patched&quot;) if the user isn't logged in,<br />
or is logged in as another non-admin user<br />
(see <code>test/controllers/users_controller_test.rb</code>)</li>
<li>users can't be destroyed if the user isn't logged in, or is logged<br />
in as another non-admin user<br />
(see <code>test/controllers/users_controller_test.rb</code>)</li>
<li>a request to show the edit user page is redirected away<br />
if the user isn't logged in, or is logged as another non-admin user -<br />
this prevents any information leak from the edit page<br />
(see <code>test/controllers/users_controller_test.rb</code>)</li>
<li>a user page does not display its email address when the user is<br />
either (1) not logged in or (2) is logged in but not as an admin.<br />
(see <code>test/controllers/users_controller_test.rb</code>)</li>
<li>a user page does not display if the user is an admin if<br />
the user isn't logged in, or is logged in as a non-admin user<br />
(see <code>test/controllers/users_controller_test.rb</code>).<br />
This makes it slightly harder for attackers to figure out<br />
the individuals to target (they have additional privileges), while<br />
still allowing <em>administrators</em> to easily see if a user has<br />
administrator privileges.</li>
</ul>
</li>
<li>The software has a strong test suite; our policy requires<br />
at least 90% statement coverage.<br />
In practice our autoamted test suite coverage is much higher; it has achieved<br />
100% statement coverage for a long time.<br />
You can verify this by looking at the &quot;CodeCov&quot; value at the<br />
<a href="https://github.com/coreinfrastructure/best-practices-badge">BadgeApp repository</a>.<br />
This strong test suite<br />
makes it easier to update components (e.g., if a third-party component<br />
has a publicly disclosed vulnerability).<br />
The test suite also makes it easier to make other fixes (e.g., to harden<br />
something) and have fairly high<br />
confidence that the change did not break functionality.<br />
It can also counter some vulnerabilities, e.g.,<br />
<a href="http://www.dwheeler.com/essays/apple-goto-fail.html#coverage">Apple's<br />
goto fail vulnerability would have been detected had they<br />
checked statement coverage</a>.</li>
</ul>
<p>We have briefly experimented with using the &quot;dawnscanner&quot; security scanner.<br />
We have decided to <em>not</em> add dawnscanner to the set of scanners that we<br />
routinely use, because it doesn't really add any value in our particular<br />
situation.<br />
See the <a href="./dawnscanner.html">dawnscanner.md</a> file for more information.</p>
<p>These steps cannot <em>guarantee</em> that there are no vulnerabilities,<br />
but we think they greatly reduce the risks.</p>
<h2><a href="#transition-and-operation" aria-hidden="true" class="anchor" id="transition-and-operation"></a>Transition and operation</h2>
<p>To be secure, the software has to be secure as actually transitioned<br />
(deployed) and operated securely.</p>
<p>Our transition process has software normally go through tiers.<br />
At this time there are two deployed tiers: staging and production.<br />
At one time we also had a &quot;main&quot; aka &quot;master&quot; tier; that ran<br />
the software at the HEAD of the main (formerly master) branch,<br />
but to reduce costs we no longer have a tier that deploys the main branch.<br />
At any time software can be promoted from the main branch<br />
to the staging tier (using <code>rake deploy_staging</code>). Software that<br />
runs fine on the staging branch is promoted to production (the &quot;real&quot; system).<br />
In an emergency we can skip tiers or promote to them in parallel, but<br />
doing that is rare.</p>
<p>Our operations process has a number of security measures.<br />
Our deployment provider and CDN provider take steps to be secure.<br />
Online checkers of our deployed site suggest that we have<br />
a secure site.<br />
In addition, we have detection and recovery processes<br />
that help us limit damage.</p>
<h3><a href="#deployment-provider" aria-hidden="true" class="anchor" id="deployment-provider"></a>Deployment provider</h3>
<p>We deploy via a cloud provider who takes a number of steps<br />
to keep our system secure.<br />
We currently use Heroku for deployment; see the<br />
<a href="https://www.heroku.com/policy/security">Heroku security policy</a><br />
for some information on how they manage security<br />
(including physical security and environmental safeguards).<br />
Normal users cannot directly access the database management system (DBMS),<br />
which on the production system is Postgres.<br />
Anyone can create a Heroku application and run it on Heroku, however,<br />
at that point we trust the Postgres developers and the Heroku administrators<br />
to keep the databases separate.</p>
<p>People can log in via GitHub accounts; in those cases we depend<br />
on GitHub to correctly authenticate users.<br />
<a href="https://help.github.com/articles/github-security/">GitHub takes steps to keep itself secure</a>.</p>
<h3><a href="#online-checkers" aria-hidden="true" class="anchor" id="online-checkers"></a><a name="online-checkers"></a>Online checkers</h3>
<p>Various online checkers give us an overall clean bill of health.<br />
Most of the checkers test our HTTPS (TLS) configuration and<br />
if common hardening mechanisms are enabled.</p>
<p>For the main bestpractices.coreinfrastructure.org site we have:</p>
<ul>
<li>An &quot;A+&quot; rating from the<br />
<a href="https://www.ssllabs.com/ssltest/analyze.html?d=bestpractices.coreinfrastructure.org">Qualys SSL labs check of our TLS configuration</a><br />
on 2017-01-14.</li>
<li>An &quot;A+&quot; rating from the<br />
<a href="https://securityheaders.io/?q=https%3A%2F%2Fbestpractices.coreinfrastructure.org">securityheaders.io check of our HTTP security headers</a><br />
on 2018-01-25.<br />
Back in 2017-01-14 securityheaders.io<br />
gave us a slightly lower score (&quot;A&quot;) because we do not include<br />
&quot;Public-Key-Pins&quot;.  This simply notes that<br />
we are do not implement HTTP Public Key Pinning (HPKP).<br />
HPKP counters rogue certificate authorities (CAs), but it also has risks.<br />
HPKP makes it harder to switch CAs <em>and</em> any error in its configuration,<br />
at any time, risks serious access problems that are unfixable -<br />
making HPKP somewhat dangerous to use.<br />
Many others have come to the same conclusion, and securityheaders.io<br />
has stopped using HPKP as a grading criterion.</li>
<li>An all-pass report from the<br />
<a href="https://www.sslshopper.com/ssl-checker.html#hostname=bestpractices.coreinfrastructure.org">SSLShopper SSL checker</a><br />
on 2017-01-14.</li>
<li>An &quot;A+&quot; rating from the <a href="https://observatory.mozilla.org/analyze.html?host=bestpractices.coreinfrastructure.org">Mozilla Observatory</a> scan summary<br />
on 2017-01-14.</li>
<li>A 96% result from <a href="https://www.wormly.com/test_ssl/h/bestpractices.coreinfrastructure.org/i/157.52.75.7/p/443">Wormly</a>.<br />
The only item not passed was the &quot;SSL Handshake Size&quot; test; the live site<br />
provides 5667 bytes, and they consider values beyond 4K (with unclear<br />
units) to be large. This is not a security issue, at most this will<br />
result in a slower initial connection.  Thus, we don't plan to worry<br />
about the missing test.</li>
</ul>
<h3><a href="#detection" aria-hidden="true" class="anchor" id="detection"></a>Detection</h3>
<p>We have various detection mechanisms to detect problems.<br />
There are two approaches to detection:</p>
<ul>
<li>internal (which has access to our internal information, such as logs)</li>
<li>external (which does not have access to internal information)</li>
</ul>
<p>We use <em>both</em> detection approaches.<br />
We tend to focus on the internal approach, which has more information<br />
available to it.<br />
The external approaches do not have access<br />
to as much information, but they see the site as a &quot;typical&quot; user<br />
would, so combining these approaches has its advantages.</p>
<h4><a href="#internal" aria-hidden="true" class="anchor" id="internal"></a>Internal</h4>
<p>This is a <a href="https://12factor.net/">12 factor app</a>; as such,<br />
events are streamed to standard out for logging.<br />
We use the &quot;rails_12factor&quot; to ensure that all Rails logs go to<br />
standard out, and then use standard Heroku logging mechanisms.<br />
The logs then go out to other components for further analysis.</p>
<p>System logs are expressly <em>not</em> publicly available.<br />
They are only shared with a small number of people authorized by the<br />
Linux Foundation, and are protected information.<br />
You must have administrator access to our Heroku site or our<br />
logging management system to gain access to the logs.<br />
That is because our system logs must include detailed information so that we<br />
can identify and fix problems (including attacks).<br />
For example, log entries record the IP address of the requestor,<br />
email addresses when we send email,<br />
and the user id (uid) making a request (if the user is logged in).<br />
We record this information so we can keep the system running properly.<br />
We also need to keep it for a period of time so we can identify trends,<br />
including slow-moving attacks.<br />
For more information, see the<br />
<a href="https://www.linuxfoundation.org/privacy">Linux Foundation privacy policy</a>.</p>
<p>As an additional protection measure, we take steps to <em>not</em> include<br />
passwords in logs.<br />
That's because people sometimes reuse passwords, so we try to be<br />
especially careful with passwords.<br />
File <code>config/initializers/filter_parameter_logging</code> expressly<br />
filters out the &quot;password&quot; field.</p>
<p>We intentionally omit here, in this public document, details about<br />
how logs are stored and how anomaly detection is done to<br />
detect and counter things.</p>
<h4><a href="#external" aria-hidden="true" class="anchor" id="external"></a>External</h4>
<p>We are also alerted if the website goes down.</p>
<p>One of those mechanisms is uptime robot:<br />
<a href="https://uptimerobot.com/dashboard">https://uptimerobot.com/dashboard</a></p>
<h3><a href="#recovery-plan-including-backups" aria-hidden="true" class="anchor" id="recovery-plan-including-backups"></a>Recovery plan including backups</h3>
<p>Once we detect that there is a problem, we have plans<br />
and mechanisms in place to help us recover,<br />
including backups.</p>
<p>Once we determine that there is a problem, we must<br />
determine to a first order the scale of the problem,<br />
what to do immediately, and what to do over time.</p>
<p>If there is an ongoing security issue, we have a few immediate options<br />
(other than just leaving the system running).<br />
We can shut down the system,<br />
disable internet access to it, or enable the<br />
application's <code>BADGEAPP_DENY_LOGIN</code> mode.<br />
The <code>BADGEAPP_DENY_LOGIN</code> mode is a special degraded mode<br />
we have pre-positioned that enables key functionality while<br />
countering some attacks.</p>
<p>We will work with the LF data controller to determine if there has been<br />
a personal data breach, and help the data controller alert the<br />
supervisory authority where appropriate.<br />
The General Data Protection Regulation (GDPR) section 33 requires that<br />
in the case of a personal data breach, the controller shall<br />
&quot;without undue delay, and, where feasible, not later than 72 hours<br />
after having become aware of it, notify the personal data breach to the<br />
supervisory authority... unless the personal data breach is unlikely to<br />
result in a risk to the rights and freedoms of natural persons.&quot;</p>
<p>Once we have determined the cause, we would work to quickly determine<br />
how to fix the software and/or change the configuration to<br />
address the attack (at least to ensure that integrity and confidentiality<br />
are maintained).<br />
As shown elsewhere, we have a variety of tools and tests that help us<br />
rapidly update the software with confidence.</p>
<p>Once the system is fixed, we might need to alert users.<br />
We have a pre-created rake task <code>mass_email</code> that lets us quickly<br />
send email to the users (or a subset) if it strictly necessary.</p>
<p>We might also need to re-encrypt the email addresses with a new key.<br />
We have pre-created a rake task &quot;rekey&quot; that performs rekeying of<br />
email addresses should that be necessary.</p>
<p>Finally, we might need to restore from backups.<br />
We use the standard Rails and PostgreSQL mechanisms for loading backups.<br />
We backup the database daily, and archive many versions so<br />
we can restore from them.<br />
See the <a href="https://devcenter.heroku.com/articles/heroku-postgres-backups#scheduled-backups-retention-limits">Heroku site</a> for retention times.</p>
<p>The update process to the &quot;staging&quot; site backs up the production site<br />
to the staging site.  This provides an additional backup, and also<br />
serves as a check to make sure the backup and restore processes are working.</p>
<h2><a href="#maintenance" aria-hidden="true" class="anchor" id="maintenance"></a>Maintenance</h2>
<p>What many call the &quot;maintenance&quot; or &quot;sustainment&quot; process is simply<br />
continuous execution of all our processes.</p>
<p>However, there is a special case related to security:<br />
detecting when publicly known vulnerabilities are reported in the<br />
components we use (in our direct or indirect dependencies),<br />
and then speedily fixing that (usually by updating the component).<br />
A component could have no publicly-known vulnerabilities when we selected it,<br />
yet one could be found later.</p>
<p>We use a variety of techniques to detect vulnerabilities that have<br />
been newly reported to the public, and have a process for<br />
rapidly responding to them, as described below.<br />
In some cases a reused component might appear vulnerable but is not;<br />
for more discussion, including specific examples, see the<br />
section on <a href="#reuse">reuse</a>.</p>
<h3><a href="#auto-detect-vulnerabilities" aria-hidden="true" class="anchor" id="auto-detect-vulnerabilities"></a>Auto-detect vulnerabilities</h3>
<p>We use multiple processes for automatically detecting when the components we<br />
use have publicly known vulnerabilities.<br />
We specifically focus on detecting all components with any publicly known<br />
vulnerability, both in our direct and indirect dependencies.</p>
<p>We detect components with publicly known vulnerabilities<br />
using 2 different mechanisms: bundle-audit and GitHub.<br />
Each approach has its advantages, and using multiple mechanisms<br />
increases the likelihood that we will be alerted quickly.<br />
These both use the <code>Gemfile*</code> files and the<br />
National Vulnerability Database aka NVD (the NVD is<br />
a widely-used database of known vulnerabilities):</p>
<ul>
<li>bundle-audit compares the entire set of gems (libraries),<br />
both direct and indirect dependencies, to a database<br />
of versions with known vulnerabilities.<br />
The default 'rake' task invokes bundle-audit, so every time we run<br />
&quot;rake&quot; (as part of our build or deploy process)<br />
we are alerted about publicly known vulnerabilities in the<br />
components we depend on (directly or not).<br />
In particular, this is part of our continuous integration test suite.</li>
<li>GitHub sends alerts to us for known security vulnerabilities found in<br />
dependencies.  This is a GitHub configuration setting<br />
(under settings, options, data services).<br />
This provides us with an immediate warning of a vulnerability,<br />
even if we are not currently modifying the system.<br />
This analyzes both Gemfile (direct) and Gemfile.lock<br />
(indirect) dependencies in Ruby.<br />
GitHub also includes dependabot, a service that automatically<br />
creates pull requests to fix vulnerable dependencies.<br />
These automatically-generated pull requests go through our CI pipeline,<br />
as would any pull request, so the pull requests go through many checks<br />
(including our test suite). Presuming they pass, we can accept the<br />
pull request, speeding our response to any vulnerabilities in components<br />
we use.<br />
For more information, see the<br />
<a href="https://help.github.com/en/github/managing-security-vulnerabilities/about-security-alerts-for-vulnerable-dependencies">GitHub page About Security Alerts for Vulnerable Dependencies</a>.</li>
</ul>
<p>At one time we also used Gemnasium, but the service we used<br />
closed in May 2018.  We have two other services, so that loss did not<br />
substantively impact us.</p>
<p>This approach has a complicated false positive with <code>omniauth</code><br />
which we've had to specially handle.<br />
Library <code>omniauth</code> has a publicly-known vulnerability CVE-2015-9284.<br />
We have chosen to counter vulnerability CVE-2015-9284 by installing a<br />
third-party countermeasure, <code>omniauth-rails_csrf_protection</code> and ensuring<br />
that our configuration counters the problem.<br />
This is the<br />
<a href="https://github.com/omniauth/omniauth/wiki/Resolving-CVE-2015-9284">recommended approach on the omniauth wiki</a><br />
given discussion on<br />
<a href="https://github.com/omniauth/omniauth/pull/809">pull request 809</a>.<br />
The omniauth developers have been reluctant to fix this within the<br />
component they develop because it's (1) a<br />
breaking (interface) change and (2) code available to fix it is Rails-specific,<br />
yet their library can be used in other situations.<br />
We reviewed the third-party countermeasure's code and it looks okay.<br />
See our commit <code>ccdd0e007ee7d6aa</code> for details.<br />
This is not ideal, but it's a real-world situation, and we believe<br />
this approach completely counters the vulnerability.</p>
<h3><a href="#rapid-update" aria-hidden="true" class="anchor" id="rapid-update"></a>Rapid update</h3>
<p>We also have a process for quickly responding to alerts<br />
of publicly known vulnerabilities, so that we can quickly update,<br />
automatically test, and ship to production once we've been<br />
alerted to a problem.  If a component we use has a known vulnerability<br />
we normally simply update and deploy quickly, instead of trying to determine<br />
if the vulnerability is exploitable in our system, because determining<br />
exploitability usually takes more effort than<br />
simply using our highly-automated update process.</p>
<p>The list of libraries we use (transitively) is managed by bundler, so<br />
updating libraries or sets of libraries can be done quickly.<br />
Bundler is a Ruby package manager, and it uses the Node Package Manager (NPM)<br />
to manage JavaScript libraries.<br />
As noted earlier, our strong automated test suite makes it easy to test this<br />
updated set, so we can rapidly update libraries, test the result, and<br />
deploy it.</p>
<p>We have also optimized the component update process through<br />
using the package manager (bundler) and high test coverage.<br />
The files Gemfile and Gemfile.lock<br />
identify the current versions of Ruby gems (Gemfile identifies direct<br />
dependencies; Gemfile.lock includes all transitive dependencies and<br />
the exact version numbers).  We can rapidly update libraries by<br />
updating those files, running &quot;bundle install&quot;, and then using &quot;rake&quot;<br />
to run various automated checks including a robust test suite.<br />
Once those pass, we can immediately field the results.</p>
<p>This approach is known to work.<br />
Commit fdb83380aa71352<br />
on 2015-11-26 updated nokogiri, in response to a bundle-audit<br />
report on advisory CVE-2015-1819, &quot;Nokogiri gem contains<br />
several vulnerabilities in libxml2 and libxslt&quot;.<br />
When it was publicly reported we were alerted.<br />
In less than an hour from the time the vulnerability<br />
was publicly reported we were alerted,<br />
updated the library, ran the full test suite, and deployed the fixed version<br />
to our production site.</p>
<p>This automatic detection and remediation<br />
process does <em>not</em> cover the underlying execution<br />
platform (e.g., kernel, system packages such as the C and Ruby runtime,<br />
and the database system (PostgreSQL)).<br />
We depend on the underlying platform provider (Heroku)<br />
to update those components and restart as necessary when<br />
a vulnerability in those components is discovered<br />
(that service is one of the key reasons we pay them!).</p>
<h2><a href="#acquisition-process" aria-hidden="true" class="anchor" id="acquisition-process"></a>Acquisition process</h2>
<p>The system depends on a deployment provider (Heroku) and<br />
content distribution network (CDN) (Fastly).<br />
We have contracts with them, which provide us with some leverage<br />
should they fail to do what they say and/or don't quickly fix<br />
security-related problems.</p>
<h2><a href="#infrastructure-management" aria-hidden="true" class="anchor" id="infrastructure-management"></a>Infrastructure management</h2>
<h3><a href="#security-of-the-development-environment" aria-hidden="true" class="anchor" id="security-of-the-development-environment"></a>Security of the development environment</h3>
<p>Subversion of the development environment can easily lead to<br />
a compromise of the resulting system.<br />
The key developers use development environments<br />
specifically configured to be secure.</p>
<p>Anyone who has direct commit rights to the repository<br />
<em>must not</em> allow other untrusted local users on the same (virtual) machine.<br />
This counters local vulnerabilities.<br />
E.g., the Rubocop vulnerability<br />
CVE-2017-8418 is /tmp file vulnerability, in which<br />
&quot;Malicious local users could exploit this to tamper<br />
with cache files belonging to other users.&quot;<br />
Since we do not allow other untrusted local users on the (virtual) machine<br />
that has commit rights, a vulnerability cannot be easily exploited<br />
this way.  If someone without commit rights submits a proposal, we can<br />
separately review that change.</p>
<p>As noted earlier, we are cautious about the components we use.<br />
The source code is managed on GitHub;<br />
<a href="https://help.github.com/articles/github-security/">GitHub takes steps to keep itself secure</a>.</p>
<p>The installation process, as described in the INSTALL.md file,<br />
includes a few steps to counter some attacks.<br />
In particular,<br />
we use the git integrity recommendations from Eric Myhre that check all<br />
git objects transferred from an external site into our development environment.<br />
This sets &quot;fsckObjects = true&quot; for transfer (thus also for fetch and receive).</p>
<h3><a href="#test-environment-lacks-protected-data" aria-hidden="true" class="anchor" id="test-environment-lacks-protected-data"></a>Test environment lacks protected data</h3>
<p>The continuous integration (CI) test environment runs on CircleCI,<br />
and does <em>not</em> have direct access to the real-world data.<br />
Thus, if someone can see the data available on the test environment,<br />
that does <em>not</em> mean that they will have access to the protected data.</p>
<h3><a href="#cicd-pipeline-input-validation-and-sanitization" aria-hidden="true" class="anchor" id="cicd-pipeline-input-validation-and-sanitization"></a>CI/CD pipeline input validation and sanitization</h3>
<p>Our CI/CD pipelines implement baseline security requirements to prevent<br />
injection attacks through pipeline parameters and branch names.<br />
Specifically, we implement:</p>
<ul>
<li>
<p><strong>OSPS-BR-01.01</strong> (Input parameter sanitization): Our CI/CD pipelines<br />
do not accept explicit user-provided input parameters. All inputs come<br />
from trusted sources (CI/CD system context variables, environment<br />
variables from secure contexts, or hardcoded configuration values).<br />
This is implemented in <code>.circleci/config.yml</code> (CircleCI) and<br />
<code>.github/workflows/*.yml</code> (GitHub Actions).</p>
</li>
<li>
<p><strong>OSPS-BR-01.02</strong> (Branch name sanitization and validation): Before using<br />
branch names in any pipeline operations, we validate them using the<br />
centralized script <code>script/validate_branch_name</code>, which uses<br />
POSIX-compliant shell code to ensure they contain only safe characters<br />
(alphanumeric, hyphens, underscores, dots, plus signs, and forward slashes)<br />
and have an appropriate length (more than 0, no more than 200 characters).<br />
We also reject branch names starting with <code>-</code> (hyphen) because they could<br />
be confused as command-line option flags, and reject branch names containing<br />
<code>..</code> to prevent directory traversal attempts.<br />
This prevents command injection and cache poisoning attacks.<br />
The validation script is called from:</p>
<ul>
<li><code>.circleci/config.yml</code>: Validates branch names in both the build job<br />
(which runs on all branches) and the deploy job (which also validates<br />
against a staging/production allow-list)</li>
<li><code>.github/workflows/main.yml</code>: Validates branch names in the main CI workflow</li>
<li><code>.github/workflows/scorecard.yml</code>: Validates branch names in the<br />
scorecard security workflow</li>
<li><code>.github/workflows/codespell.yml</code>: Validates branch names in the<br />
codespell workflow</li>
</ul>
</li>
</ul>
<p>These validations provide defense in depth by ensuring that even if<br />
workflow-level filters were bypassed or misconfigured, the pipeline<br />
would fail fast and refuse to execute with potentially malicious input.<br />
The use of POSIX-compliant code (avoiding bash-specific extensions)<br />
ensures maximum portability and standards compliance.</p>
<h2><a href="#human-resource-management--people" aria-hidden="true" class="anchor" id="human-resource-management--people"></a>Human resource management  (people)</h2>
<p>ISO/IEC/IEEE 12207 has a &quot;human resource management&quot; process;<br />
this is the process that focuses on the people involved.<br />
Of course, it's important to have developers who know how to develop software,<br />
with at least someone in the group who knows how to develop secure software.</p>
<p>The lead software developer,<br />
<a href="http://www.dwheeler.com/">David A. Wheeler</a>, is an expert in the area<br />
of developing secure software.<br />
He has a PhD in Information Technology, a Master's degree in Computer Science,<br />
a certificate in Software Engineering, a certificate in<br />
Information Systems Security, and a BS in Electronics Engineering,<br />
all from George Mason University (GMU).<br />
He wrote the book<br />
<a href="http://www.dwheeler.com/secure-programs/">Secure Programming HOWTO</a><br />
and teaches a graduate course at George Mason University (GMU) on<br />
how to design and implement secure software.<br />
Dr. Wheeler's doctoral dissertation,<br />
<a href="http://www.dwheeler.com/trusting-trust/">Fully Countering Trusting Trust through Diverse Double-Compiling</a>,<br />
discusses how to counter malicious compilers.</p>
<p>Sam Khakimov was greatly involved in its earlier development.<br />
He has been developing software for a number of years,<br />
in a variety of languages.<br />
He has a Bachelor of Business Admin in Finance and Mathematics<br />
(CUNY Baruch College Summa Cum Laude Double Major) and a<br />
Master of Science in Mathematics (New York University) with<br />
additional coursework in Cyber Security.</p>
<p><a href="http://www.dankohn.com/bio.html">Dan Kohn</a><br />
received a bachelor's degree in Economics and Computer Science<br />
from the Honors program of Swarthmore College.<br />
He has long expertise in Ruby on Rails.</p>
<p>Jason Dossett has a PhD in Physics from The University of Texas at Dallas,<br />
and has been involved in software development for many years.<br />
He has reviewed and is familiar with the security assurance case<br />
provided here.</p>
<h2><a href="#project-planning" aria-hidden="true" class="anchor" id="project-planning"></a>Project planning</h2>
<p>We plan development, and always consider security as we develop new plans.</p>
<h2><a href="#risk-management" aria-hidden="true" class="anchor" id="risk-management"></a>Risk management</h2>
<p>The primary risk we're concerned about is security, so we have developed<br />
the assurance case here to determine how to counter that risk.</p>
<h2><a href="#quality-assurance" aria-hidden="true" class="anchor" id="quality-assurance"></a>Quality assurance</h2>
<p>We continuously review our processes and their results to see if there<br />
are systemic problems, and if so, try to address them.<br />
In particular, we try to maximize automation, including automated tests<br />
and automated security analysis, to reduce the risk that the deployed<br />
system will produce incorrect results or will be insecure.</p>
<h2><a href="#configuration-management" aria-hidden="true" class="anchor" id="configuration-management"></a>Configuration management</h2>
<p>See the <a href="./governance.html">governance</a> document<br />
for information on how the project is governed, including<br />
how important changes are controlled.</p>
<p>For version control we use git, a widely-used<br />
distributed version control system.</p>
<p>As noted in the requirements section,<br />
modifications to the official BadgeApp application require<br />
authentication via GitHub.<br />
We use GitHub for managing the source code and issue tracker; it<br />
has an authentication system for this purpose.</p>
<h2><a href="#certifications-and-controls" aria-hidden="true" class="anchor" id="certifications-and-controls"></a>Certifications and Controls</h2>
<h3><a href="#certifications-receive-cii-best-practices-badge" aria-hidden="true" class="anchor" id="certifications-receive-cii-best-practices-badge"></a>Certifications (receive CII best practices badge)</h3>
<p>One way to increase confidence in an application is to pass<br />
relevant certifcations.  In our case, the BadgeApp is the result<br />
of an OSS project, so a useful measure is to receive our own<br />
CII best practices badge.</p>
<p>The CII best practices badging project was established to identify<br />
best practices that can lead to more secure software.<br />
The BadgeApp application achieves its own badge.<br />
This is evidence that the BadgeApp application is<br />
applying practices expected in a well-run FLOSS project.</p>
<p>You can see the<br />
<a href="https://bestpractices.coreinfrastructure.org/en/projects/1/0">CII Best Practices Badge entry for the BadgeApp</a>.<br />
Note that we achieve a gold badge.</p>
<h3><a href="#organizational-controls" aria-hidden="true" class="anchor" id="organizational-controls"></a>Organizational Controls</h3>
<p>The<br />
<a href="https://www.cisecurity.org/controls/">Center for Internet Security (CIS) Controls</a><br />
are a &quot;prioritized set of actions to protect your organization and data<br />
from known cyber attack vectors.&quot;<br />
They are used and supported in many places, e.g.,<br />
<a href="https://www.sans.org/critical-security-controls">SANS Supports the CIS Critical Security Controls</a><br />
through a number of resources and information security courses.</p>
<p>Here we compare the CIS controls to the deployed BadgeApp application.<br />
The CIS controls are intended for an entire organization,<br />
while we are focusing on deployment of a single application,<br />
so many controls don't make sense in our context.<br />
Still, it's still useful to go through a set of<br />
controls to make sure we are covering what's important.</p>
<p>For now we only examine the &quot;first 5 CIS controls&quot;, as these<br />
&quot;eliminate the vast majority of your organization's vulnerabilities&quot;.</p>
<p>Here are the top CIS controls:</p>
<ol>
<li>Inventory of Authorized and Unauthorized Devices.<br />
The application is deployed on Heroku, who perform the task to<br />
actively manage (inventory, track, and correct)<br />
all hardware devices on the network.</li>
<li>Inventory of Authorized and Unauthorized Software.<br />
The deployed system only has one main authorized program, and only<br />
authorized administrators can deploy or change software on the system.<br />
The operating system (including kernel and utilities) and<br />
database system are managed by Heroku.<br />
We manage the application; its subcomponents are managed by<br />
package managers.<br />
We do not use an allowlisting mechanism because it would be pointless;<br />
at no time does the running system download software to run<br />
(not even JavaScript), and only administrators can install software on it.</li>
<li>Secure Configurations for Hardware and Software on<br />
Mobile Devices, Laptops, Workstations, and Servers.<br />
The hardware and the operating system (including kernel and utilities)<br />
by Heroku; see <a href="https://www.heroku.com/policy/security">Heroku security</a>.<br />
For the rest we follow strict configuration management processes;<br />
images (builds) are created using CircleCI and deployed to Heroku<br />
over an authenticated and encrypted channel.<br />
These configurations are extensively hardened as described above, and we<br />
use package management systems to rigorously manage and control updates.</li>
<li>Continuous Vulnerability Assessment and Remediation.<br />
We use automated vulnerability scanning tools to<br />
alert us to publicly-known vulnerabilities in third-party<br />
components via multiple mechanisms, as described above.<br />
We remediate and minimize the opportunity for attack by using<br />
package managers to rapidly perform controlled updates, combined<br />
with a large automated test suite to rapidly check that the system<br />
continues to work.<br />
These scans focus on the reused software components in our application.<br />
CIS recommends that you use a<br />
<a href="https://nvd.nist.gov/scap/validated-tools">SCAP-validated vulnerability scanner</a><br />
that looks for both code-based vulnerabilities (such as those described by<br />
Common Vulnerabilities and Exposures entries) and configuration-based<br />
vulnerabilities (as enumerated by the Common Configuration Enumeration<br />
Project).  Our system runs on the widely-used Ubuntu OS,<br />
and <a href="https://www.open-scap.org">Open SCAP</a> is the only OSS scanner<br />
of that kind that we know of - and it is only SCAP-validated<br />
for Red Hat.  In addition, we do not control the operating system<br />
(Heroku does), so it's not clear that such scanners would do us much good.<br />
However, in the future, we may attempt to try to use Open SCAP<br />
as an additional tool.</li>
<li>Controlled Use of Administrative Privileges.<br />
We minimize administrative privileges and only use administrative<br />
accounts when they are required.<br />
Administrative privileges are strictly limited to a very few<br />
people who are authorized to do so.<br />
We do not have &quot;root&quot; privileges on Heroku (Heroku handles that),<br />
so we cannot give those privileges away, and that greatly reduces the<br />
need to &quot;implement focused auditing on the use of<br />
administrative privileged functions and monitor for anomalous behavior.&quot;<br />
We do not use automated tools to inventory all administrative<br />
accounts and validate each person.<br />
We have a single production system, with only a few administrators,<br />
so automation is not useful in this situation.</li>
</ol>
<h2><a href="#residual-risks" aria-hidden="true" class="anchor" id="residual-risks"></a>Residual risks</h2>
<p>It is not possible to eliminate all risks.<br />
Here are a few of the more-prominent residual risks, and why we<br />
believe they are acceptable:</p>
<ul>
<li><em>External service dependencies.</em><br />
We depend on several external services, and if they are subverted<br />
(externally or by an insider) then our service might be subverted as well.<br />
The most obvious services we depend on are GitHub, Heroku, and<br />
Amazon Web Services.<br />
We use GitHub to maintain the code, and we depend on GitHub to<br />
authenticate GitHub users.  The website itself runs on Heroku, and<br />
Heroku in turn depends on Amazon Web Services.<br />
However, these services have generally good reputations, are<br />
professionally-managed, and have a history of careful monitoring and<br />
rapid response to any issue.  It's not obvious that we would do<br />
better if we did it ourselves.</li>
<li><em>Third party components.</em><br />
As discussed earlier, like all real systems we depend on a large number<br />
of third party components we did not develop.  These components<br />
could have unintentional or even intentional vulnerabilities.<br />
However, recreating them would cost far more time, and since we can make<br />
mistakes too it's unlikely that the result would be better. Instead,<br />
as discussed above, we apply a variety of techniques to manage our risks.<br />
For example, we check any direct dependency we might add before adding it,<br />
we auto-detect vulnerabilities (that have become publicly known), and<br />
have a process that supports rapid update.</li>
<li><em>DDoS.</em><br />
We use a variety of techniques to reduce the impact of DDoS attacks.<br />
These include using a scalable cloud service,<br />
using a Content Delivery Network (CDN), and requiring<br />
the system to return to operation quickly after<br />
a DDoS attack has ended.<br />
For more information, see the discussion on availability<br />
in the requirements section (above).<br />
However, DDoS attacks are fundamentally resource-on-resource attacks,<br />
so if an attack is powerful enough, we can only counter it by also<br />
pouring in lots of resources (which is expensive).<br />
The same is true for almost any other website.</li>
<li><em>Keys are stored in environment variables and processed by the application.</em><br />
We use the very common approach of storing keys<br />
(such as encryption keys) in environment variables, and use<br />
the application software to apply them.<br />
This means that an attacker who subverts the entire application or<br />
underlying system could acquire copies of the keys.<br />
This includes an attacker who could get a raw copy of a memory dump -<br />
in such a case, the attacker could see the keys.<br />
A stronger countermeasure would be to store keys in hardware devices,<br />
or at least completely separate isolated applications, and then do<br />
processing with keys in that separate execution environment.<br />
However, this is more complex to deal with, and we've decided it<br />
just isn't necessary in our circumstance.  We do partly counter this<br />
by making it easy for us to change keys.</li>
<li><em>A vulnerability we missed.</em><br />
Perfection is hard to achieve.<br />
We have considered security throughout system development,<br />
analyzed it for security issues, and documented what we've determined<br />
in this assurance case.<br />
That said, we could still have missed a vulnerability.<br />
We have released the information so that others can review it,<br />
and published a vulnerability report handling process so that<br />
security analysts can report findings to us.<br />
We believe we've addressed security enough to deploy the system.</li>
</ul>
<h2><a href="#vulnerability-report-handling-process" aria-hidden="true" class="anchor" id="vulnerability-report-handling-process"></a>Vulnerability report handling process</h2>
<p>As noted in CONTRIBUTING.md, if anyone finds a<br />
significant vulnerability, or evidence of one, we ask that they<br />
send that information to at least one of the security contacts.<br />
The CONTRIBUTING.md file explains how to report a vulnerability;<br />
below we describe what happens once vulnerability is reported.</p>
<p>Whoever receives that report will share that information with the<br />
other security contacts, and one of them will analyze it:</p>
<ul>
<li>If is not valid, one of the security contacts will reply back to<br />
the reporter to explain that (if this is a misunderstanding,<br />
the reporter can reply and start the process again).</li>
<li>If it is a bug but not security vulnerability, the security contact<br />
will create an issue as usual for repair.</li>
<li>If it is a security vulnerability, one of the security contacts will<br />
fix it in a <em>local</em> git repository and <em>not</em> share it with the world<br />
until the fix is ready.  An issue will <em>not</em> be filed, since those<br />
are public.  If it needs review, the review will not be public.<br />
Discussions will be held, as much as practical, using encrypted<br />
channels (e.g., using email systems that support hop-to-hop encryption).<br />
Once the fix is ready, it will be quickly moved through all tiers.<br />
The goal is to minimize the risk of attackers exploiting the problem<br />
before it is fixed.  Our goal is to fix any real vulnerability within<br />
two calendar weeks of a report (and do it faster if practical);<br />
the actual time will depend on the difficulty of repair.</li>
</ul>
<p>Once the fix is in the final production system, credit will be<br />
publicly given to the vulnerability reporter (unless the reporter<br />
requested otherwise).</p>
<h2><a href="#why-are-we-using-sacm-graphical-notation" aria-hidden="true" class="anchor" id="why-are-we-using-sacm-graphical-notation"></a>Why are we using SACM graphical notation?</h2>
<p>Historically we used<br />
<a href="https://www.adelard.com/asce/choosing-asce/cae.html">Claims- Arguments- Evidence (CAE) notation</a>,<br />
CAE notation is wonderfully simple:<br />
Claims (including subclaims) are ovals,<br />
arguments are rounded rectangles, and evidence (references) are rectangles.<br />
The GSN alternative was too complex (it uses many more symbols) and confusing<br />
to non-experts (e.g., it uses terms like &quot;Strategy&quot; for an argument).<br />
In addition, when we started the SACM graphical notation did not exist<br />
(it was not released until 2020).</p>
<p>Historically the<br />
<a href="https://www.omg.org/spec/SACM/About-SACM/">Object Management Group (OMG) Structured Assurance Case Metamodel (SACM) specification</a><br />
specification has focused on defining a standard interchange<br />
format for assurance case data.<br />
<a href="https://www.adelard.com/asce/choosing-asce/standardisation.html">Mappings are available between other notations and the SACM data structures</a>,<br />
However, we currently aren’t trying to exchange with other systems.<br />
So while we didn't know of anything intrinsically wrong with SACM,<br />
historically the SACM specification wasn’t<br />
focused on solving any problems we’re trying to solve.<br />
In addition, we don’t know of any mature OSS tools that directly<br />
support the SACM data format.<br />
By policy, any tools we <em>depend</em> on must be OSS, and when we modify the<br />
code or configuration we must be able to update the assurance case.</p>
<p>However, in 2020 version 2.1 of SACM added a new graphical notation.<br />
Claims (including subclaims) are rectangles, ArgumentReasoning (aka<br />
arguments) are open rectangles, ArtifactCitations (used for evidence)<br />
are shadowed rectangles, and a connection showing an AssertedRelationship<br />
use intermediate symbols such as &quot;big dots&quot;.</p>
<p>After comparing the notations, we have found that the SACM<br />
graphical notation has many advantages over CAE graphical notation:</p>
<ol>
<li>CAE Claim vs. SACM Claim. CAE uses ovals, while SACM uses<br />
rectangles. SACM has a <em>BIG</em> win here: Rectangles use MUCH less<br />
space, so complex diagrams are much easier to create &amp; easier to<br />
understand.</li>
<li>CAE Argument vs. SACM ArgumentReasoning. CAE uses rounded<br />
rectangles, while SACM uses a shape I’ll call an &quot;open rectangle”.<br />
CAE’s rounded rectangles are not very distinct from its evidence<br />
rectangles, which is a minor negative for the CAE notation. SACM<br />
initially presented some challenges when using our drawing tool<br />
(LIbreOffice Draw), but I overcame them:</li>
</ol>
<ul>
<li>SACM’s half-rectangle initially presented me with a problem:<br />
that is <em>NOT</em> a built-in shape for the drawing tool I’m using<br />
(LIbreOffice Draw). I suspect it’s not a built-in symbol in many<br />
tools. I was able to work around this by creating a polygon (many<br />
drawing tools support this, and this is a very easy polygon to<br />
make). It took a little tweaking, but I managed to create a simple<br />
polygon with embedded text. In the longer term, the SACM community<br />
should work to get this easy icon into other drawing tools, to<br />
simplify its use.</li>
<li>SACM’s half-rectangle is VERY hard to visually distinguish<br />
if both it &amp; claims are filled with color. I use color fills<br />
to help the eye notice type differences. My solution was simple:<br />
color fill everything <em>except</em> the half-rectangle; this makes<br />
them all visually distinct.</li>
</ul>
<ol start="3">
<li>CAE Evidence vs. SACM ArtifactReference.<br />
In CAE this is a simple rectangle. In SACM this is a shadowed<br />
rectangle with an arrow; the arrow is hard to add with simple<br />
drawing tools, but the shadow is trivial to add with a “shadow”<br />
property in LibreOffice (and many other drawing tools), and I<br />
think just the shadow is adequate. The shadow adds slightly more<br />
space (but MUCH less than ovals), and it takes a moment to draw<br />
by hand, but I think that’s a reasonable trade-off to ensure<br />
that they are visually distinct. In addition: I tend to record<br />
evidence / ArtifactReferences in <em>only</em> text, not in the diagrams,<br />
because diagrams are time-consuming to maintain. So making<br />
<em>claims</em> simple to draw, and making evidence/ArtifactReferences<br />
slightly more complex to draw, is exactly the right tradeoff.</li>
<li>Visual distinctiveness. In <em>general</em> the CAE icons<br />
for Claim/Argument/Evidence are not as visually distinct as<br />
SACM’s Claim/ArgumentReasoning/ArtifactReference, especially<br />
when they get shaped to the text contents. That’s an overall<br />
advantage for the SACM graphical notation.</li>
<li>SACM’s “bigdot”.<br />
The bigdot, e.g., in AssertedInference, make the diagrams simpler<br />
by making it easy to move an argument / ArgumentReasoning icon<br />
away from the flow from supporting claims/evidence to a higher<br />
claim. It also makes the arrows clearer (you merge flows earlier)<br />
and that merging makes combinations easier to follow.<br />
You could also informally do that with CAE, but it’s clearly a part of SACM.</li>
<li>It has an asCited notation that a claim in one location is described<br />
elsewhere, a rectangle in another (an asCited Claim). I've been using &quot;See...&quot;<br />
as text instead.</li>
</ol>
<p>In our SACM diagrams we've sometimes omitted the bigdot when there is a<br />
single connection from one element to another.<br />
This is not strictly compliant.<br />
One advantage of using “bigdot” even in these<br />
cases would be that it would make it much easier to add<br />
an ArgumentReasoning later.<br />
However, adding the bigdot in those cases is nontrivial<br />
extra work when using our basic drawing tools (because we must draw<br />
the bigdot, two connectors, and connect them all up,<br />
instead of using a simple direct connection).</p>
<p>We don't have an easy way to display SACM's ArgumentPackage and related<br />
symbols. We just use a &quot;scroll&quot; icon in that case to indicate<br />
each diagram package.</p>
<p>At first it appeared that there was a problem with<br />
SACM’s ArgumentReasoning symbol, rectangle open on one side.<br />
While it’s easy to connect on the<br />
left/top/bottom, it’s somewhat unclear when trying to connect from<br />
its bare right-hand-side in its presented orientation<br />
(because the lines are not visibly connected to another symbol).<br />
My thanks to Scott Ankrum for pointing this out!<br />
This not an unusual problem; in data flow diagrams, the &quot;data store&quot;<br />
symbol is open on both the left and right edges<br />
(resulting in the same problem).<br />
One solution, without changing anything, is to prefer to put<br />
this icon on the right-hand-side of what it connects to.<br />
Another solution would have been to use another symbol, e.g., an<br />
an uneven pentagon (“pointer”) or callout symbol (with the little tail).<br />
However, the paper<br />
&quot;A Visual Notation for the Representation of Assurance Cases using SACM&quot;<br />
suggests a simpler solution was intended: horizontally flip the<br />
rectangle open on one side.</p>
<h2><a href="#future-work" aria-hidden="true" class="anchor" id="future-work"></a>Future work</h2>
<p><a href="https://resources.sei.cmu.edu/asset_files/TechnicalNote/2010_004_001_15176.pdf">&quot;Evaluating and Mitigating Software Supply Chain Security Risks&quot; by Ellison et al, May 2020</a> has an assurance case focusing on<br />
supply chain security risks, as well as other information.<br />
We intend to review it for future ideas.</p>
<h2><a href="#your-help-is-welcome" aria-hidden="true" class="anchor" id="your-help-is-welcome"></a>Your help is welcome!</h2>
<p>Security is hard; we welcome your help.<br />
We welcome hardening in general, particularly pull requests<br />
that actually do the work of hardening.<br />
We thank many, including Reg Meeson, for reviewing and providing feedback<br />
on this assurance case.</p>
<p>Please report potential vulnerabilities you find; see<br />
<a href="../CONTRIBUTING.html">CONTRIBUTING.md</a> for how to submit a vulnerability report.</p>
<h2><a href="#see-also-1" aria-hidden="true" class="anchor" id="see-also-1"></a>See also</h2>
<p>Project participation and interface:</p>
<ul>
<li><a href="../CONTRIBUTING.html">CONTRIBUTING.md</a> - How to contribute to this project</li>
<li><a href="INSTALL.html">INSTALL.md</a> - How to install/quick start</li>
<li><a href="governance.html">governance.md</a> - How the project is governed</li>
<li><a href="roadmap.html">roadmap.md</a> - Overall direction of the project</li>
<li><a href="background.html">background.md</a> - Background research</li>
<li><a href="api.html">api</a> - Application Programming Interface (API), inc. data downloads</li>
</ul>
<p>Criteria:</p>
<ul>
<li><a href="https://bestpractices.coreinfrastructure.org/criteria/0">Criteria for passing badge</a></li>
<li><a href="https://bestpractices.coreinfrastructure.org/criteria">Criteria for all badge levels</a></li>
</ul>
<p>Development processes and security:</p>
<ul>
<li><a href="requirements.html">requirements.md</a> - Requirements (what's it supposed to do?)</li>
<li><a href="design.html">design.md</a> - Architectural design information</li>
<li><a href="implementation.html">implementation.md</a> - Implementation notes</li>
<li><a href="testing.html">testing.md</a> - Information on testing</li>
<li><a href="assurance-case.html">assurance-case.md</a> - Why it's adequately secure (assurance case)</li>
</ul>
  </body>
  </html>
